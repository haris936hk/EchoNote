{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# \ud83c\udfaf EchoNote Model Evaluation Suite\n",
        "\n",
        "**Comprehensive evaluation of the fine-tuned EchoNote meeting summarization model.**\n",
        "\n",
        "## Evaluation Metrics\n",
        "\n",
        "| Metric | Description |\n",
        "|--------|-------------|\n",
        "| **Semantic Similarity** | Cosine similarity between generated and ground truth embeddings |\n",
        "| **Format Compliance** | Valid JSON structure with all required fields |\n",
        "| **Content Coverage** | Key information extraction completeness |\n",
        "| **Action Items** | Quality of extracted action items (task, assignee, deadline, priority) |\n",
        "| **NER Precision** | Accuracy of named entity recognition (people, orgs, dates) |\n",
        "| **Sentiment Accuracy** | Correct sentiment classification |\n",
        "| **Length Compliance** | Executive summary within target length range |\n",
        "| **Topic Relevance** | Alignment of extracted topics with transcript content |\n",
        "\n",
        "**Model:** `haris936hk/echonote`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install dependencies\n",
        "!pip install -q transformers>=4.40.0 accelerate>=0.27.0 bitsandbytes>=0.42.0\n",
        "!pip install -q torch>=2.0.0\n",
        "!pip install -q sentence-transformers>=2.5.0\n",
        "!pip install -q spacy>=3.7.0 textblob>=0.18.0\n",
        "!pip install -q scikit-learn>=1.4.0 numpy>=1.24.0 pandas>=2.0.0\n",
        "!pip install -q matplotlib>=3.8.0 seaborn>=0.13.0\n",
        "!pip install -q tqdm>=4.66.0 jsonschema>=4.21.0\n",
        "!python -m spacy download en_core_web_lg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import random\n",
        "import warnings\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "from dataclasses import dataclass, field\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from jsonschema import validate, ValidationError\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"\u2705 All imports successful!\")\n",
        "print(f\"\ud83d\udd27 PyTorch version: {torch.__version__}\")\n",
        "print(f\"\ud83d\udd27 CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\ud83d\udd27 GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class EvalConfig:\n",
        "    \"\"\"Evaluation configuration\"\"\"\n",
        "    # Model settings (merged model - not LoRA adapters)\n",
        "    model_id: str = \"haris936hk/echonote\"\n",
        "    \n",
        "    # Dataset settings\n",
        "    dataset_path: str = \"echonote_dataset_combined.jsonl\"\n",
        "    test_size: int = 100  # Number of samples to evaluate\n",
        "    random_seed: int = 42\n",
        "    \n",
        "    # Generation settings\n",
        "    max_new_tokens: int = 512\n",
        "    temperature: float = 0.1\n",
        "    top_p: float = 0.9\n",
        "    \n",
        "    # Length constraints (from training)\n",
        "    min_summary_chars: int = 150\n",
        "    max_summary_chars: int = 600\n",
        "    \n",
        "    # Embedding model for semantic similarity\n",
        "    embedding_model: str = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "config = EvalConfig()\n",
        "print(f\"\ud83d\udcca Evaluation Config:\")\n",
        "print(f\"   Model: {config.model_id}\")\n",
        "print(f\"   Test samples: {config.test_size}\")\n",
        "print(f\"   Random seed: {config.random_seed}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "schema"
      },
      "outputs": [],
      "source": [
        "# Expected output schema\n",
        "OUTPUT_SCHEMA = {\n",
        "    \"type\": \"object\",\n",
        "    \"required\": [\"executiveSummary\", \"keyDecisions\", \"actionItems\", \"nextSteps\", \"keyTopics\", \"sentiment\"],\n",
        "    \"properties\": {\n",
        "        \"executiveSummary\": {\"type\": \"string\", \"minLength\": 150},\n",
        "        \"keyDecisions\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "        \"actionItems\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"required\": [\"task\", \"assignee\", \"deadline\", \"priority\"],\n",
        "                \"properties\": {\n",
        "                    \"task\": {\"type\": \"string\"},\n",
        "                    \"assignee\": {\"type\": \"string\"},\n",
        "                    \"deadline\": {\"type\": \"string\"},\n",
        "                    \"priority\": {\"type\": \"string\", \"enum\": [\"high\", \"medium\", \"low\"]}\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"nextSteps\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "        \"keyTopics\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "        \"sentiment\": {\"type\": \"string\", \"enum\": [\"positive\", \"neutral\", \"negative\"]}\n",
        "    }\n",
        "}\n",
        "\n",
        "# System prompt (same as training)\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert meeting intelligence assistant.\n",
        "\n",
        "Your task is to read a meeting transcript and produce a concise but comprehensive structured summary.\n",
        "\n",
        "Follow these rules strictly:\n",
        "1. Read the transcript carefully and identify key participants, topics, decisions, and action items.\n",
        "2. The overall sentiment should be inferred from the tone and content of the meeting.\n",
        "3. Output must be valid JSON matching the schema exactly.\n",
        "4. Use this exact structure:\n",
        "{\n",
        "  \"executiveSummary\": string,\n",
        "  \"keyDecisions\": string[],\n",
        "  \"actionItems\": [\n",
        "    {\n",
        "      \"task\": string,\n",
        "      \"assignee\": string,\n",
        "      \"deadline\": string,\n",
        "      \"priority\": \"high\" | \"medium\" | \"low\"\n",
        "    }\n",
        "  ],\n",
        "  \"nextSteps\": string[],\n",
        "  \"keyTopics\": string[],\n",
        "  \"sentiment\": \"positive\" | \"neutral\" | \"negative\"\n",
        "}\n",
        "\n",
        "5. The \"executiveSummary\" must be a well-written narrative paragraph of AT LEAST 150 characters that accurately reflects the discussion and context of the meeting.\n",
        "6. If there are no explicit decisions, action items, or next steps, return an EMPTY ARRAY for those fields.\n",
        "7. Do NOT invent decisions, tasks, or deadlines that were not discussed or clearly implied.\n",
        "8. \"actionItems\" must be concrete and assigned ONLY when responsibility is clear.\n",
        "9. \"keyTopics\" should list the main discussion themes using short phrases.\n",
        "10. \"sentiment\" must reflect the overall tone of the meeting, using transcript content and the provided NLP sentiment cues.\n",
        "\n",
        "All required fields must be present.\n",
        "Empty arrays are allowed when applicable.\n",
        "Output must be valid JSON and nothing else.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\u2705 Schema and system prompt defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Model & Resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "print(\"\ud83d\udd04 Loading fine-tuned EchoNote model (merged)...\")\n",
        "\n",
        "# Quantization config for efficient inference\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# Load merged model directly (not base + adapters)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_id, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(f\"\u2705 Model loaded successfully!\")\n",
        "print(f\"   Model: {config.model_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_resources"
      },
      "outputs": [],
      "source": [
        "# Load embedding model for semantic similarity\n",
        "print(\"\ud83d\udd04 Loading embedding model...\")\n",
        "embedding_model = SentenceTransformer(config.embedding_model)\n",
        "print(f\"\u2705 Embedding model loaded: {config.embedding_model}\")\n",
        "\n",
        "# Load spaCy for NER\n",
        "print(\"\ud83d\udd04 Loading spaCy model...\")\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "print(\"\u2705 spaCy model loaded: en_core_web_lg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load & Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_dataset"
      },
      "outputs": [],
      "source": [
        "def load_dataset(path: str) -> List[Dict]:\n",
        "    \"\"\"Load JSONL dataset\"\"\"\n",
        "    data = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "def parse_ground_truth(output_str: str) -> Optional[Dict]:\n",
        "    \"\"\"Parse ground truth JSON from output string\"\"\"\n",
        "    try:\n",
        "        return json.loads(output_str)\n",
        "    except:\n",
        "        match = re.search(r'\\{[\\s\\S]*\\}', output_str)\n",
        "        if match:\n",
        "            try:\n",
        "                return json.loads(match.group())\n",
        "            except:\n",
        "                pass\n",
        "    return None\n",
        "\n",
        "# Load full dataset\n",
        "print(f\"\ud83d\udd04 Loading dataset from {config.dataset_path}...\")\n",
        "full_dataset = load_dataset(config.dataset_path)\n",
        "print(f\"\u2705 Loaded {len(full_dataset)} samples\")\n",
        "\n",
        "# Sample test set\n",
        "random.seed(config.random_seed)\n",
        "test_indices = random.sample(range(len(full_dataset)), min(config.test_size, len(full_dataset)))\n",
        "test_dataset = [full_dataset[i] for i in test_indices]\n",
        "\n",
        "print(f\"\ud83d\udcca Test set: {len(test_dataset)} samples\")\n",
        "\n",
        "# Validate ground truth\n",
        "valid_samples = []\n",
        "for sample in test_dataset:\n",
        "    gt = parse_ground_truth(sample['output'])\n",
        "    if gt:\n",
        "        valid_samples.append({'input': sample['input'], 'ground_truth': gt})\n",
        "\n",
        "print(f\"\u2705 Valid samples with parseable ground truth: {len(valid_samples)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Generation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generation"
      },
      "outputs": [],
      "source": [
        "def generate_summary(input_text: str) -> Tuple[str, Optional[Dict]]:\n",
        "    \"\"\"\n",
        "    Generate meeting summary using the fine-tuned model.\n",
        "    Returns: (raw_output, parsed_json)\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": input_text}\n",
        "    ]\n",
        "    \n",
        "    # Apply chat template\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=config.max_new_tokens,\n",
        "            temperature=config.temperature,\n",
        "            top_p=config.top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode only new tokens\n",
        "    raw_output = tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    ).strip()\n",
        "    \n",
        "    # Parse JSON\n",
        "    parsed = None\n",
        "    try:\n",
        "        # Try direct parse\n",
        "        parsed = json.loads(raw_output)\n",
        "    except:\n",
        "        # Try to extract JSON from response\n",
        "        match = re.search(r'\\{[\\s\\S]*\\}', raw_output)\n",
        "        if match:\n",
        "            try:\n",
        "                parsed = json.loads(match.group())\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    return raw_output, parsed\n",
        "\n",
        "# Test generation\n",
        "print(\"\ud83e\uddea Testing generation with first sample...\")\n",
        "test_raw, test_parsed = generate_summary(valid_samples[0]['input'])\n",
        "print(f\"\u2705 Generation successful!\")\n",
        "print(f\"   Raw output length: {len(test_raw)} chars\")\n",
        "print(f\"   Parsed JSON: {'Yes' if test_parsed else 'No'}\")\n",
        "if test_parsed:\n",
        "    print(f\"   Keys: {list(test_parsed.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluation Metrics Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "metrics_class"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class EvaluationResult:\n",
        "    \"\"\"Container for all evaluation metrics\"\"\"\n",
        "    # Core metrics\n",
        "    semantic_similarity: float = 0.0\n",
        "    format_compliance: float = 0.0\n",
        "    content_coverage: float = 0.0\n",
        "    action_items_score: float = 0.0\n",
        "    ner_precision: float = 0.0\n",
        "    sentiment_accuracy: float = 0.0\n",
        "    length_compliance: float = 0.0\n",
        "    topic_relevance: float = 0.0\n",
        "    \n",
        "    # Detailed breakdowns\n",
        "    format_details: Dict = field(default_factory=dict)\n",
        "    action_items_details: Dict = field(default_factory=dict)\n",
        "    ner_details: Dict = field(default_factory=dict)\n",
        "    \n",
        "    def overall_score(self) -> float:\n",
        "        \"\"\"Weighted average of all metrics\"\"\"\n",
        "        weights = {\n",
        "            'semantic_similarity': 0.20,\n",
        "            'format_compliance': 0.15,\n",
        "            'content_coverage': 0.15,\n",
        "            'action_items_score': 0.15,\n",
        "            'ner_precision': 0.10,\n",
        "            'sentiment_accuracy': 0.10,\n",
        "            'length_compliance': 0.05,\n",
        "            'topic_relevance': 0.10\n",
        "        }\n",
        "        total = sum(\n",
        "            getattr(self, metric) * weight \n",
        "            for metric, weight in weights.items()\n",
        "        )\n",
        "        return total\n",
        "\n",
        "print(\"\u2705 EvaluationResult class defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "metric_functions"
      },
      "outputs": [],
      "source": [
        "class EchoNoteEvaluator:\n",
        "    \"\"\"Comprehensive evaluator for EchoNote model outputs\"\"\"\n",
        "    \n",
        "    def __init__(self, embedding_model, nlp_model, schema):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.nlp = nlp_model\n",
        "        self.schema = schema\n",
        "    \n",
        "    def evaluate_semantic_similarity(self, generated: Dict, ground_truth: Dict) -> float:\n",
        "        \"\"\"\n",
        "        Metric 1: Semantic Similarity\n",
        "        Compares embeddings of executive summaries and overall content.\n",
        "        \"\"\"\n",
        "        if not generated or not ground_truth:\n",
        "            return 0.0\n",
        "        \n",
        "        # Compare executive summaries\n",
        "        gen_summary = generated.get('executiveSummary', '')\n",
        "        gt_summary = ground_truth.get('executiveSummary', '')\n",
        "        \n",
        "        if not gen_summary or not gt_summary:\n",
        "            return 0.0\n",
        "        \n",
        "        # Get embeddings\n",
        "        gen_emb = self.embedding_model.encode([gen_summary])\n",
        "        gt_emb = self.embedding_model.encode([gt_summary])\n",
        "        \n",
        "        # Cosine similarity\n",
        "        similarity = cosine_similarity(gen_emb, gt_emb)[0][0]\n",
        "        \n",
        "        # Also compare key topics\n",
        "        gen_topics = ' '.join(generated.get('keyTopics', []))\n",
        "        gt_topics = ' '.join(ground_truth.get('keyTopics', []))\n",
        "        \n",
        "        if gen_topics and gt_topics:\n",
        "            topics_emb_gen = self.embedding_model.encode([gen_topics])\n",
        "            topics_emb_gt = self.embedding_model.encode([gt_topics])\n",
        "            topics_sim = cosine_similarity(topics_emb_gen, topics_emb_gt)[0][0]\n",
        "            similarity = 0.7 * similarity + 0.3 * topics_sim\n",
        "        \n",
        "        return float(max(0, similarity))\n",
        "    \n",
        "    def evaluate_format_compliance(self, generated: Dict, raw_output: str) -> Tuple[float, Dict]:\n",
        "        \"\"\"\n",
        "        Metric 2: Format Compliance\n",
        "        Checks if output is valid JSON with all required fields.\n",
        "        \"\"\"\n",
        "        details = {\n",
        "            'is_valid_json': False,\n",
        "            'has_all_required_fields': False,\n",
        "            'schema_valid': False,\n",
        "            'field_scores': {}\n",
        "        }\n",
        "        \n",
        "        if generated is None:\n",
        "            return 0.0, details\n",
        "        \n",
        "        details['is_valid_json'] = True\n",
        "        \n",
        "        # Check required fields\n",
        "        required = ['executiveSummary', 'keyDecisions', 'actionItems', 'nextSteps', 'keyTopics', 'sentiment']\n",
        "        present_fields = [f for f in required if f in generated]\n",
        "        details['has_all_required_fields'] = len(present_fields) == len(required)\n",
        "        \n",
        "        # Field-by-field scoring\n",
        "        for field in required:\n",
        "            if field in generated:\n",
        "                value = generated[field]\n",
        "                if field == 'executiveSummary':\n",
        "                    details['field_scores'][field] = 1.0 if isinstance(value, str) and len(value) >= 50 else 0.5\n",
        "                elif field == 'sentiment':\n",
        "                    details['field_scores'][field] = 1.0 if value in ['positive', 'neutral', 'negative'] else 0.0\n",
        "                elif field == 'actionItems':\n",
        "                    if isinstance(value, list):\n",
        "                        valid_items = sum(1 for item in value if all(k in item for k in ['task', 'assignee', 'deadline', 'priority']))\n",
        "                        details['field_scores'][field] = valid_items / max(len(value), 1) if value else 1.0\n",
        "                    else:\n",
        "                        details['field_scores'][field] = 0.0\n",
        "                else:\n",
        "                    details['field_scores'][field] = 1.0 if isinstance(value, list) else 0.5\n",
        "            else:\n",
        "                details['field_scores'][field] = 0.0\n",
        "        \n",
        "        # Schema validation\n",
        "        try:\n",
        "            validate(instance=generated, schema=self.schema)\n",
        "            details['schema_valid'] = True\n",
        "        except ValidationError:\n",
        "            details['schema_valid'] = False\n",
        "        \n",
        "        # Calculate score\n",
        "        score = 0.0\n",
        "        score += 0.3 if details['is_valid_json'] else 0.0\n",
        "        score += 0.3 if details['has_all_required_fields'] else 0.0\n",
        "        score += 0.2 if details['schema_valid'] else 0.0\n",
        "        score += 0.2 * (sum(details['field_scores'].values()) / len(required))\n",
        "        \n",
        "        return score, details\n",
        "    \n",
        "    def evaluate_content_coverage(self, generated: Dict, ground_truth: Dict, input_text: str) -> float:\n",
        "        \"\"\"\n",
        "        Metric 3: Content Coverage\n",
        "        Measures how well the generated output covers key information from input.\n",
        "        \"\"\"\n",
        "        if not generated:\n",
        "            return 0.0\n",
        "        \n",
        "        # Extract entities and key phrases from input\n",
        "        doc = self.nlp(input_text[:5000])  # Limit for performance\n",
        "        \n",
        "        input_entities = set(ent.text.lower() for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'MONEY', 'PERCENT', 'DATE'])\n",
        "        input_numbers = set(re.findall(r'\\$?[\\d,]+\\.?\\d*[%MKB]?', input_text))\n",
        "        \n",
        "        # Flatten generated content\n",
        "        gen_text = json.dumps(generated).lower()\n",
        "        gt_text = json.dumps(ground_truth).lower() if ground_truth else ''\n",
        "        \n",
        "        # Coverage scores\n",
        "        entity_coverage = sum(1 for e in input_entities if e in gen_text) / max(len(input_entities), 1)\n",
        "        number_coverage = sum(1 for n in input_numbers if n.lower() in gen_text) / max(len(input_numbers), 1)\n",
        "        \n",
        "        # Compare list lengths\n",
        "        gen_decisions = len(generated.get('keyDecisions', []))\n",
        "        gt_decisions = len(ground_truth.get('keyDecisions', [])) if ground_truth else 0\n",
        "        \n",
        "        gen_actions = len(generated.get('actionItems', []))\n",
        "        gt_actions = len(ground_truth.get('actionItems', [])) if ground_truth else 0\n",
        "        \n",
        "        decisions_ratio = min(gen_decisions, gt_decisions) / max(gt_decisions, 1) if gt_decisions else (1.0 if gen_decisions > 0 else 0.5)\n",
        "        actions_ratio = min(gen_actions, gt_actions) / max(gt_actions, 1) if gt_actions else (1.0 if gen_actions > 0 else 0.5)\n",
        "        \n",
        "        # Weighted score\n",
        "        score = 0.3 * entity_coverage + 0.2 * number_coverage + 0.25 * decisions_ratio + 0.25 * actions_ratio\n",
        "        \n",
        "        return min(1.0, score)\n",
        "    \n",
        "    def evaluate_action_items(self, generated: Dict, ground_truth: Dict) -> Tuple[float, Dict]:\n",
        "        \"\"\"\n",
        "        Metric 4: Action Items Quality\n",
        "        Evaluates completeness and quality of action items.\n",
        "        \"\"\"\n",
        "        details = {\n",
        "            'generated_count': 0,\n",
        "            'ground_truth_count': 0,\n",
        "            'complete_items': 0,\n",
        "            'valid_priorities': 0,\n",
        "            'has_deadlines': 0,\n",
        "            'has_assignees': 0\n",
        "        }\n",
        "        \n",
        "        if not generated:\n",
        "            return 0.0, details\n",
        "        \n",
        "        gen_items = generated.get('actionItems', [])\n",
        "        gt_items = ground_truth.get('actionItems', []) if ground_truth else []\n",
        "        \n",
        "        details['generated_count'] = len(gen_items)\n",
        "        details['ground_truth_count'] = len(gt_items)\n",
        "        \n",
        "        if not gen_items:\n",
        "            # If ground truth also has no items, it's correct\n",
        "            return 1.0 if not gt_items else 0.3, details\n",
        "        \n",
        "        for item in gen_items:\n",
        "            if not isinstance(item, dict):\n",
        "                continue\n",
        "            \n",
        "            has_task = bool(item.get('task', '').strip())\n",
        "            has_assignee = bool(item.get('assignee', '').strip())\n",
        "            has_deadline = bool(item.get('deadline', '').strip())\n",
        "            has_priority = item.get('priority', '') in ['high', 'medium', 'low']\n",
        "            \n",
        "            if has_task and has_assignee and has_deadline and has_priority:\n",
        "                details['complete_items'] += 1\n",
        "            if has_priority:\n",
        "                details['valid_priorities'] += 1\n",
        "            if has_deadline:\n",
        "                details['has_deadlines'] += 1\n",
        "            if has_assignee:\n",
        "                details['has_assignees'] += 1\n",
        "        \n",
        "        # Calculate score\n",
        "        n = len(gen_items)\n",
        "        completeness = details['complete_items'] / n\n",
        "        priority_score = details['valid_priorities'] / n\n",
        "        deadline_score = details['has_deadlines'] / n\n",
        "        assignee_score = details['has_assignees'] / n\n",
        "        \n",
        "        # Count alignment\n",
        "        count_ratio = min(details['generated_count'], details['ground_truth_count']) / max(details['ground_truth_count'], 1)\n",
        "        \n",
        "        score = 0.3 * completeness + 0.2 * priority_score + 0.2 * deadline_score + 0.2 * assignee_score + 0.1 * min(count_ratio, 1.0)\n",
        "        \n",
        "        return score, details\n",
        "    \n",
        "    def evaluate_ner_precision(self, generated: Dict, input_text: str) -> Tuple[float, Dict]:\n",
        "        \"\"\"\n",
        "        Metric 5: NER Precision\n",
        "        Checks if named entities in output actually appear in input.\n",
        "        \"\"\"\n",
        "        details = {\n",
        "            'persons_mentioned': [],\n",
        "            'persons_valid': 0,\n",
        "            'orgs_mentioned': [],\n",
        "            'orgs_valid': 0,\n",
        "            'dates_mentioned': [],\n",
        "            'dates_valid': 0\n",
        "        }\n",
        "        \n",
        "        if not generated:\n",
        "            return 0.0, details\n",
        "        \n",
        "        input_lower = input_text.lower()\n",
        "        gen_text = json.dumps(generated)\n",
        "        doc = self.nlp(gen_text)\n",
        "        \n",
        "        persons = []\n",
        "        orgs = []\n",
        "        dates = []\n",
        "        \n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == 'PERSON':\n",
        "                persons.append(ent.text)\n",
        "            elif ent.label_ == 'ORG':\n",
        "                orgs.append(ent.text)\n",
        "            elif ent.label_ in ['DATE', 'TIME']:\n",
        "                dates.append(ent.text)\n",
        "        \n",
        "        # Also extract from action items assignees\n",
        "        for item in generated.get('actionItems', []):\n",
        "            if isinstance(item, dict) and item.get('assignee'):\n",
        "                persons.append(item['assignee'])\n",
        "        \n",
        "        persons = list(set(persons))\n",
        "        orgs = list(set(orgs))\n",
        "        \n",
        "        details['persons_mentioned'] = persons\n",
        "        details['orgs_mentioned'] = orgs\n",
        "        details['dates_mentioned'] = dates\n",
        "        \n",
        "        # Check validity\n",
        "        for person in persons:\n",
        "            # Check if any part of name appears in input\n",
        "            name_parts = person.lower().split()\n",
        "            if any(part in input_lower for part in name_parts if len(part) > 2):\n",
        "                details['persons_valid'] += 1\n",
        "        \n",
        "        for org in orgs:\n",
        "            if org.lower() in input_lower or any(word.lower() in input_lower for word in org.split() if len(word) > 3):\n",
        "                details['orgs_valid'] += 1\n",
        "        \n",
        "        # Calculate precision\n",
        "        person_precision = details['persons_valid'] / max(len(persons), 1)\n",
        "        org_precision = details['orgs_valid'] / max(len(orgs), 1)\n",
        "        \n",
        "        # Weight persons more heavily (assignees are important)\n",
        "        score = 0.7 * person_precision + 0.3 * org_precision\n",
        "        \n",
        "        return score, details\n",
        "    \n",
        "    def evaluate_sentiment_accuracy(self, generated: Dict, ground_truth: Dict) -> float:\n",
        "        \"\"\"\n",
        "        Metric 6: Sentiment Accuracy\n",
        "        Checks if sentiment matches ground truth.\n",
        "        \"\"\"\n",
        "        if not generated or not ground_truth:\n",
        "            return 0.0\n",
        "        \n",
        "        gen_sentiment = generated.get('sentiment', '').lower()\n",
        "        gt_sentiment = ground_truth.get('sentiment', '').lower()\n",
        "        \n",
        "        valid_sentiments = ['positive', 'neutral', 'negative']\n",
        "        \n",
        "        if gen_sentiment not in valid_sentiments:\n",
        "            return 0.0\n",
        "        \n",
        "        if gen_sentiment == gt_sentiment:\n",
        "            return 1.0\n",
        "        \n",
        "        # Partial credit for adjacent sentiments\n",
        "        sentiment_order = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "        if gen_sentiment in sentiment_order and gt_sentiment in sentiment_order:\n",
        "            diff = abs(sentiment_order[gen_sentiment] - sentiment_order[gt_sentiment])\n",
        "            if diff == 1:\n",
        "                return 0.5  # Adjacent sentiment\n",
        "        \n",
        "        return 0.0\n",
        "    \n",
        "    def evaluate_length_compliance(self, generated: Dict) -> float:\n",
        "        \"\"\"\n",
        "        Metric 7: Length Compliance\n",
        "        Checks if executive summary is within target length.\n",
        "        \"\"\"\n",
        "        if not generated:\n",
        "            return 0.0\n",
        "        \n",
        "        summary = generated.get('executiveSummary', '')\n",
        "        if not summary:\n",
        "            return 0.0\n",
        "        \n",
        "        length = len(summary)\n",
        "        \n",
        "        # Target: 150-600 characters\n",
        "        min_len = config.min_summary_chars\n",
        "        max_len = config.max_summary_chars\n",
        "        \n",
        "        if min_len <= length <= max_len:\n",
        "            return 1.0\n",
        "        elif length < min_len:\n",
        "            # Penalty for too short\n",
        "            return max(0, length / min_len)\n",
        "        else:\n",
        "            # Smaller penalty for too long (still contains info)\n",
        "            return max(0.5, 1.0 - (length - max_len) / max_len)\n",
        "    \n",
        "    def evaluate_topic_relevance(self, generated: Dict, input_text: str) -> float:\n",
        "        \"\"\"\n",
        "        Metric 8: Topic Relevance\n",
        "        Checks if extracted topics are relevant to the transcript.\n",
        "        \"\"\"\n",
        "        if not generated:\n",
        "            return 0.0\n",
        "        \n",
        "        topics = generated.get('keyTopics', [])\n",
        "        if not topics:\n",
        "            return 0.3  # Partial credit if no topics extracted\n",
        "        \n",
        "        input_lower = input_text.lower()\n",
        "        \n",
        "        # Extract important words from input\n",
        "        doc = self.nlp(input_text[:5000])\n",
        "        input_nouns = set(token.lemma_.lower() for token in doc if token.pos_ in ['NOUN', 'PROPN'] and len(token.text) > 3)\n",
        "        \n",
        "        # Check topic relevance\n",
        "        relevant_topics = 0\n",
        "        for topic in topics:\n",
        "            topic_words = topic.lower().split()\n",
        "            # Check if topic words appear in input\n",
        "            matches = sum(1 for w in topic_words if w in input_lower or w in input_nouns)\n",
        "            if matches >= len(topic_words) * 0.5:  # At least half the words match\n",
        "                relevant_topics += 1\n",
        "        \n",
        "        return relevant_topics / len(topics)\n",
        "    \n",
        "    def evaluate_sample(self, input_text: str, raw_output: str, generated: Dict, ground_truth: Dict) -> EvaluationResult:\n",
        "        \"\"\"Run all evaluations on a single sample\"\"\"\n",
        "        result = EvaluationResult()\n",
        "        \n",
        "        # 1. Semantic Similarity\n",
        "        result.semantic_similarity = self.evaluate_semantic_similarity(generated, ground_truth)\n",
        "        \n",
        "        # 2. Format Compliance\n",
        "        result.format_compliance, result.format_details = self.evaluate_format_compliance(generated, raw_output)\n",
        "        \n",
        "        # 3. Content Coverage\n",
        "        result.content_coverage = self.evaluate_content_coverage(generated, ground_truth, input_text)\n",
        "        \n",
        "        # 4. Action Items\n",
        "        result.action_items_score, result.action_items_details = self.evaluate_action_items(generated, ground_truth)\n",
        "        \n",
        "        # 5. NER Precision\n",
        "        result.ner_precision, result.ner_details = self.evaluate_ner_precision(generated, input_text)\n",
        "        \n",
        "        # 6. Sentiment Accuracy\n",
        "        result.sentiment_accuracy = self.evaluate_sentiment_accuracy(generated, ground_truth)\n",
        "        \n",
        "        # 7. Length Compliance\n",
        "        result.length_compliance = self.evaluate_length_compliance(generated)\n",
        "        \n",
        "        # 8. Topic Relevance\n",
        "        result.topic_relevance = self.evaluate_topic_relevance(generated, input_text)\n",
        "        \n",
        "        return result\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = EchoNoteEvaluator(embedding_model, nlp, OUTPUT_SCHEMA)\n",
        "print(\"\u2705 EchoNoteEvaluator initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Run Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_eval"
      },
      "outputs": [],
      "source": [
        "def run_full_evaluation(samples: List[Dict], evaluator: EchoNoteEvaluator) -> Tuple[List[EvaluationResult], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Run evaluation on all samples and return results.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    detailed_records = []\n",
        "    \n",
        "    print(f\"\ud83d\ude80 Starting evaluation on {len(samples)} samples...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for i, sample in enumerate(tqdm(samples, desc=\"Evaluating\")):\n",
        "        input_text = sample['input']\n",
        "        ground_truth = sample['ground_truth']\n",
        "        \n",
        "        # Generate output\n",
        "        try:\n",
        "            raw_output, generated = generate_summary(input_text)\n",
        "        except Exception as e:\n",
        "            print(f\"\\n\u274c Generation failed for sample {i}: {e}\")\n",
        "            generated = None\n",
        "            raw_output = \"\"\n",
        "        \n",
        "        # Evaluate\n",
        "        result = evaluator.evaluate_sample(input_text, raw_output, generated, ground_truth)\n",
        "        results.append(result)\n",
        "        \n",
        "        # Record details\n",
        "        detailed_records.append({\n",
        "            'sample_id': i,\n",
        "            'semantic_similarity': result.semantic_similarity,\n",
        "            'format_compliance': result.format_compliance,\n",
        "            'content_coverage': result.content_coverage,\n",
        "            'action_items_score': result.action_items_score,\n",
        "            'ner_precision': result.ner_precision,\n",
        "            'sentiment_accuracy': result.sentiment_accuracy,\n",
        "            'length_compliance': result.length_compliance,\n",
        "            'topic_relevance': result.topic_relevance,\n",
        "            'overall_score': result.overall_score(),\n",
        "            'json_valid': result.format_details.get('is_valid_json', False),\n",
        "            'gt_sentiment': ground_truth.get('sentiment', 'unknown'),\n",
        "            'gen_sentiment': generated.get('sentiment', 'unknown') if generated else 'failed'\n",
        "        })\n",
        "        \n",
        "        # Progress update every 10 samples\n",
        "        if (i + 1) % 10 == 0:\n",
        "            avg_score = np.mean([r.overall_score() for r in results])\n",
        "            print(f\"\\n\ud83d\udcca Progress: {i+1}/{len(samples)} | Avg Score: {avg_score:.3f}\")\n",
        "    \n",
        "    df = pd.DataFrame(detailed_records)\n",
        "    return results, df\n",
        "\n",
        "# Run evaluation\n",
        "results, results_df = run_full_evaluation(valid_samples, evaluator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Results Analysis & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aggregate_results"
      },
      "outputs": [],
      "source": [
        "def compute_aggregate_metrics(results: List[EvaluationResult]) -> Dict:\n",
        "    \"\"\"Compute aggregate statistics for all metrics\"\"\"\n",
        "    metrics = [\n",
        "        'semantic_similarity', 'format_compliance', 'content_coverage',\n",
        "        'action_items_score', 'ner_precision', 'sentiment_accuracy',\n",
        "        'length_compliance', 'topic_relevance'\n",
        "    ]\n",
        "    \n",
        "    aggregates = {}\n",
        "    for metric in metrics:\n",
        "        values = [getattr(r, metric) for r in results]\n",
        "        aggregates[metric] = {\n",
        "            'mean': np.mean(values),\n",
        "            'std': np.std(values),\n",
        "            'min': np.min(values),\n",
        "            'max': np.max(values),\n",
        "            'median': np.median(values)\n",
        "        }\n",
        "    \n",
        "    # Overall scores\n",
        "    overall_scores = [r.overall_score() for r in results]\n",
        "    aggregates['overall'] = {\n",
        "        'mean': np.mean(overall_scores),\n",
        "        'std': np.std(overall_scores),\n",
        "        'min': np.min(overall_scores),\n",
        "        'max': np.max(overall_scores),\n",
        "        'median': np.median(overall_scores)\n",
        "    }\n",
        "    \n",
        "    return aggregates\n",
        "\n",
        "aggregates = compute_aggregate_metrics(results)\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\ud83d\udcca ECHONOTE MODEL EVALUATION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTotal samples evaluated: {len(results)}\")\n",
        "print(f\"Model: {config.model_id}\")\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(f\"{'Metric':<25} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for metric, stats in aggregates.items():\n",
        "    print(f\"{metric:<25} {stats['mean']:>10.3f} {stats['std']:>10.3f} {stats['min']:>10.3f} {stats['max']:>10.3f}\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n\ud83c\udfaf OVERALL SCORE: {aggregates['overall']['mean']:.3f} (\u00b1{aggregates['overall']['std']:.3f})\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualizations"
      },
      "outputs": [],
      "source": [
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "# 1. Metric Comparison Bar Chart\n",
        "ax1 = axes[0, 0]\n",
        "metrics = list(aggregates.keys())[:-1]  # Exclude 'overall'\n",
        "means = [aggregates[m]['mean'] for m in metrics]\n",
        "stds = [aggregates[m]['std'] for m in metrics]\n",
        "\n",
        "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(metrics)))\n",
        "bars = ax1.bar(range(len(metrics)), means, yerr=stds, capsize=5, color=colors, edgecolor='black', alpha=0.8)\n",
        "ax1.set_xticks(range(len(metrics)))\n",
        "ax1.set_xticklabels([m.replace('_', '\\n') for m in metrics], rotation=45, ha='right', fontsize=9)\n",
        "ax1.set_ylabel('Score', fontsize=11)\n",
        "ax1.set_title('Metric Comparison (Mean \u00b1 Std)', fontsize=13, fontweight='bold')\n",
        "ax1.set_ylim(0, 1.1)\n",
        "ax1.axhline(y=aggregates['overall']['mean'], color='red', linestyle='--', label=f\"Overall: {aggregates['overall']['mean']:.3f}\")\n",
        "ax1.legend()\n",
        "\n",
        "# 2. Score Distribution\n",
        "ax2 = axes[0, 1]\n",
        "overall_scores = [r.overall_score() for r in results]\n",
        "ax2.hist(overall_scores, bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "ax2.axvline(x=np.mean(overall_scores), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(overall_scores):.3f}')\n",
        "ax2.axvline(x=np.median(overall_scores), color='orange', linestyle='--', linewidth=2, label=f'Median: {np.median(overall_scores):.3f}')\n",
        "ax2.set_xlabel('Overall Score', fontsize=11)\n",
        "ax2.set_ylabel('Frequency', fontsize=11)\n",
        "ax2.set_title('Overall Score Distribution', fontsize=13, fontweight='bold')\n",
        "ax2.legend()\n",
        "\n",
        "# 3. Radar Chart\n",
        "ax3 = axes[1, 0]\n",
        "ax3.remove()\n",
        "ax3 = fig.add_subplot(2, 2, 3, projection='polar')\n",
        "\n",
        "angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
        "angles += angles[:1]  # Complete the loop\n",
        "values = means + means[:1]\n",
        "\n",
        "ax3.plot(angles, values, 'o-', linewidth=2, color='steelblue')\n",
        "ax3.fill(angles, values, alpha=0.25, color='steelblue')\n",
        "ax3.set_xticks(angles[:-1])\n",
        "ax3.set_xticklabels([m.replace('_', '\\n') for m in metrics], fontsize=8)\n",
        "ax3.set_ylim(0, 1)\n",
        "ax3.set_title('Metric Radar Chart', fontsize=13, fontweight='bold', pad=20)\n",
        "\n",
        "# 4. Sentiment Accuracy Breakdown\n",
        "ax4 = axes[1, 1]\n",
        "sentiment_correct = sum(1 for r in results if r.sentiment_accuracy == 1.0)\n",
        "sentiment_partial = sum(1 for r in results if 0 < r.sentiment_accuracy < 1.0)\n",
        "sentiment_wrong = sum(1 for r in results if r.sentiment_accuracy == 0.0)\n",
        "\n",
        "sentiment_data = [sentiment_correct, sentiment_partial, sentiment_wrong]\n",
        "sentiment_labels = ['Correct', 'Partial', 'Wrong']\n",
        "colors_pie = ['#2ecc71', '#f39c12', '#e74c3c']\n",
        "ax4.pie(sentiment_data, labels=sentiment_labels, autopct='%1.1f%%', colors=colors_pie, startangle=90)\n",
        "ax4.set_title('Sentiment Classification Accuracy', fontsize=13, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('echonote_evaluation_results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udcc8 Visualization saved to 'echonote_evaluation_results.png'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heatmap"
      },
      "outputs": [],
      "source": [
        "# Correlation heatmap between metrics\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "metric_cols = [\n",
        "    'semantic_similarity', 'format_compliance', 'content_coverage',\n",
        "    'action_items_score', 'ner_precision', 'sentiment_accuracy',\n",
        "    'length_compliance', 'topic_relevance', 'overall_score'\n",
        "]\n",
        "\n",
        "corr_matrix = results_df[metric_cols].corr()\n",
        "\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
        "sns.heatmap(\n",
        "    corr_matrix, \n",
        "    mask=mask,\n",
        "    annot=True, \n",
        "    fmt='.2f', \n",
        "    cmap='RdYlGn',\n",
        "    center=0,\n",
        "    square=True,\n",
        "    linewidths=0.5,\n",
        "    xticklabels=[m.replace('_', '\\n') for m in metric_cols],\n",
        "    yticklabels=[m.replace('_', '\\n') for m in metric_cols]\n",
        ")\n",
        "plt.title('Metric Correlation Heatmap', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('echonote_correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udcc8 Correlation heatmap saved to 'echonote_correlation_heatmap.png'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "detailed_breakdown"
      },
      "outputs": [],
      "source": [
        "# Detailed breakdown analysis\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\ud83d\udccb DETAILED METRIC BREAKDOWN\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Format Compliance Details\n",
        "json_valid_count = sum(1 for r in results if r.format_details.get('is_valid_json', False))\n",
        "schema_valid_count = sum(1 for r in results if r.format_details.get('schema_valid', False))\n",
        "all_fields_count = sum(1 for r in results if r.format_details.get('has_all_required_fields', False))\n",
        "\n",
        "print(f\"\\n\ud83d\udcc4 FORMAT COMPLIANCE:\")\n",
        "print(f\"   Valid JSON outputs: {json_valid_count}/{len(results)} ({100*json_valid_count/len(results):.1f}%)\")\n",
        "print(f\"   Schema compliant: {schema_valid_count}/{len(results)} ({100*schema_valid_count/len(results):.1f}%)\")\n",
        "print(f\"   All required fields: {all_fields_count}/{len(results)} ({100*all_fields_count/len(results):.1f}%)\")\n",
        "\n",
        "# Action Items Details\n",
        "total_gen_actions = sum(r.action_items_details.get('generated_count', 0) for r in results)\n",
        "total_gt_actions = sum(r.action_items_details.get('ground_truth_count', 0) for r in results)\n",
        "complete_actions = sum(r.action_items_details.get('complete_items', 0) for r in results)\n",
        "\n",
        "print(f\"\\n\ud83d\udccc ACTION ITEMS:\")\n",
        "print(f\"   Total generated: {total_gen_actions}\")\n",
        "print(f\"   Total in ground truth: {total_gt_actions}\")\n",
        "print(f\"   Complete items (all fields): {complete_actions}\")\n",
        "print(f\"   Avg per sample: {total_gen_actions/len(results):.1f}\")\n",
        "\n",
        "# NER Details\n",
        "total_persons = sum(len(r.ner_details.get('persons_mentioned', [])) for r in results)\n",
        "valid_persons = sum(r.ner_details.get('persons_valid', 0) for r in results)\n",
        "\n",
        "print(f\"\\n\ud83d\udc64 NAMED ENTITY RECOGNITION:\")\n",
        "print(f\"   Persons mentioned: {total_persons}\")\n",
        "print(f\"   Persons validated: {valid_persons}\")\n",
        "print(f\"   Person precision: {100*valid_persons/max(total_persons, 1):.1f}%\")\n",
        "\n",
        "# Sentiment Breakdown\n",
        "print(f\"\\n\ud83d\ude0a SENTIMENT ANALYSIS:\")\n",
        "print(f\"   Correct: {sentiment_correct}/{len(results)} ({100*sentiment_correct/len(results):.1f}%)\")\n",
        "print(f\"   Partial match: {sentiment_partial}/{len(results)} ({100*sentiment_partial/len(results):.1f}%)\")\n",
        "print(f\"   Incorrect: {sentiment_wrong}/{len(results)} ({100*sentiment_wrong/len(results):.1f}%)\")\n",
        "\n",
        "# Length Compliance\n",
        "length_compliant = sum(1 for r in results if r.length_compliance == 1.0)\n",
        "print(f\"\\n\ud83d\udccf LENGTH COMPLIANCE:\")\n",
        "print(f\"   Within target range: {length_compliant}/{len(results)} ({100*length_compliant/len(results):.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Sample Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sample_analysis"
      },
      "outputs": [],
      "source": [
        "# Show best and worst performing samples\n",
        "overall_scores = [r.overall_score() for r in results]\n",
        "sorted_indices = np.argsort(overall_scores)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\ud83c\udfc6 TOP 5 BEST PERFORMING SAMPLES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for idx in sorted_indices[-5:][::-1]:\n",
        "    r = results[idx]\n",
        "    print(f\"\\n\ud83d\udcca Sample {idx}: Overall Score = {r.overall_score():.3f}\")\n",
        "    print(f\"   Semantic: {r.semantic_similarity:.3f} | Format: {r.format_compliance:.3f} | Content: {r.content_coverage:.3f}\")\n",
        "    print(f\"   Actions: {r.action_items_score:.3f} | NER: {r.ner_precision:.3f} | Sentiment: {r.sentiment_accuracy:.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\u26a0\ufe0f BOTTOM 5 WORST PERFORMING SAMPLES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for idx in sorted_indices[:5]:\n",
        "    r = results[idx]\n",
        "    print(f\"\\n\ud83d\udcca Sample {idx}: Overall Score = {r.overall_score():.3f}\")\n",
        "    print(f\"   Semantic: {r.semantic_similarity:.3f} | Format: {r.format_compliance:.3f} | Content: {r.content_coverage:.3f}\")\n",
        "    print(f\"   Actions: {r.action_items_score:.3f} | NER: {r.ner_precision:.3f} | Sentiment: {r.sentiment_accuracy:.3f}\")\n",
        "    \n",
        "    # Identify main issues\n",
        "    issues = []\n",
        "    if r.format_compliance < 0.5:\n",
        "        issues.append(\"JSON parsing issues\")\n",
        "    if r.semantic_similarity < 0.5:\n",
        "        issues.append(\"Low semantic match\")\n",
        "    if r.action_items_score < 0.5:\n",
        "        issues.append(\"Poor action items\")\n",
        "    if r.sentiment_accuracy < 0.5:\n",
        "        issues.append(\"Wrong sentiment\")\n",
        "    \n",
        "    if issues:\n",
        "        print(f\"   Issues: {', '.join(issues)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Export Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export"
      },
      "outputs": [],
      "source": [
        "# Export detailed results to CSV\n",
        "results_df.to_csv('echonote_evaluation_detailed.csv', index=False)\n",
        "print(\"\\n\ud83d\udcbe Detailed results saved to 'echonote_evaluation_detailed.csv'\")\n",
        "\n",
        "# Export summary report\n",
        "summary_report = {\n",
        "    'model_id': config.model_id,\n",
        "    \n",
        "    'samples_evaluated': len(results),\n",
        "    'evaluation_date': pd.Timestamp.now().isoformat(),\n",
        "    'metrics': aggregates,\n",
        "    'format_compliance_rate': json_valid_count / len(results),\n",
        "    'schema_compliance_rate': schema_valid_count / len(results),\n",
        "    'sentiment_accuracy_rate': sentiment_correct / len(results)\n",
        "}\n",
        "\n",
        "with open('echonote_evaluation_summary.json', 'w') as f:\n",
        "    json.dump(summary_report, f, indent=2, default=float)\n",
        "print(\"\ud83d\udcbe Summary report saved to 'echonote_evaluation_summary.json'\")\n",
        "\n",
        "# Display final summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\u2705 EVALUATION COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n\ud83c\udfaf Final Overall Score: {aggregates['overall']['mean']:.3f} (\u00b1{aggregates['overall']['std']:.3f})\")\n",
        "print(f\"\\n\ud83d\udcca Key Metrics:\")\n",
        "print(f\"   \u2022 Semantic Similarity: {aggregates['semantic_similarity']['mean']:.3f}\")\n",
        "print(f\"   \u2022 Format Compliance: {aggregates['format_compliance']['mean']:.3f}\")\n",
        "print(f\"   \u2022 Content Coverage: {aggregates['content_coverage']['mean']:.3f}\")\n",
        "print(f\"   \u2022 Action Items Score: {aggregates['action_items_score']['mean']:.3f}\")\n",
        "print(f\"   \u2022 NER Precision: {aggregates['ner_precision']['mean']:.3f}\")\n",
        "print(f\"   \u2022 Sentiment Accuracy: {aggregates['sentiment_accuracy']['mean']:.3f}\")\n",
        "print(f\"   \u2022 Length Compliance: {aggregates['length_compliance']['mean']:.3f}\")\n",
        "print(f\"   \u2022 Topic Relevance: {aggregates['topic_relevance']['mean']:.3f}\")\n",
        "print(\"\\n\" + \"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Interactive Sample Inspection (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "interactive"
      },
      "outputs": [],
      "source": [
        "def inspect_sample(sample_idx: int):\n",
        "    \"\"\"Inspect a specific sample in detail\"\"\"\n",
        "    if sample_idx >= len(valid_samples):\n",
        "        print(f\"\u274c Sample index {sample_idx} out of range. Max: {len(valid_samples)-1}\")\n",
        "        return\n",
        "    \n",
        "    sample = valid_samples[sample_idx]\n",
        "    result = results[sample_idx]\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"\ud83d\udccb SAMPLE {sample_idx} INSPECTION\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcca SCORES:\")\n",
        "    print(f\"   Overall: {result.overall_score():.3f}\")\n",
        "    print(f\"   Semantic Similarity: {result.semantic_similarity:.3f}\")\n",
        "    print(f\"   Format Compliance: {result.format_compliance:.3f}\")\n",
        "    print(f\"   Content Coverage: {result.content_coverage:.3f}\")\n",
        "    print(f\"   Action Items: {result.action_items_score:.3f}\")\n",
        "    print(f\"   NER Precision: {result.ner_precision:.3f}\")\n",
        "    print(f\"   Sentiment: {result.sentiment_accuracy:.3f}\")\n",
        "    print(f\"   Length: {result.length_compliance:.3f}\")\n",
        "    print(f\"   Topic Relevance: {result.topic_relevance:.3f}\")\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcdd INPUT (first 500 chars):\")\n",
        "    print(f\"   {sample['input'][:500]}...\")\n",
        "    \n",
        "    print(f\"\\n\ud83c\udfaf GROUND TRUTH SUMMARY:\")\n",
        "    print(f\"   {sample['ground_truth'].get('executiveSummary', 'N/A')[:300]}...\")\n",
        "    \n",
        "    # Generate fresh output for inspection\n",
        "    raw_output, generated = generate_summary(sample['input'])\n",
        "    \n",
        "    print(f\"\\n\ud83e\udd16 GENERATED SUMMARY:\")\n",
        "    if generated:\n",
        "        print(f\"   {generated.get('executiveSummary', 'N/A')[:300]}...\")\n",
        "        print(f\"\\n   Sentiment: {generated.get('sentiment', 'N/A')} (GT: {sample['ground_truth'].get('sentiment', 'N/A')})\")\n",
        "        print(f\"   Action Items: {len(generated.get('actionItems', []))} (GT: {len(sample['ground_truth'].get('actionItems', []))})\")\n",
        "        print(f\"   Key Topics: {generated.get('keyTopics', [])}\")\n",
        "    else:\n",
        "        print(f\"   \u274c Failed to parse JSON\")\n",
        "        print(f\"   Raw output: {raw_output[:500]}...\")\n",
        "\n",
        "# Example: Inspect sample 0\n",
        "inspect_sample(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "conclusion"
      },
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
        "\u2551                    ECHONOTE EVALUATION COMPLETE                      \u2551\n",
        "\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n",
        "\u2551                                                                      \u2551\n",
        "\u2551  \ud83d\udcca Files Generated:                                                 \u2551\n",
        "\u2551     \u2022 echonote_evaluation_detailed.csv  (per-sample results)         \u2551\n",
        "\u2551     \u2022 echonote_evaluation_summary.json  (aggregate metrics)          \u2551\n",
        "\u2551     \u2022 echonote_evaluation_results.png   (visualizations)             \u2551\n",
        "\u2551     \u2022 echonote_correlation_heatmap.png  (metric correlations)        \u2551\n",
        "\u2551                                                                      \u2551\n",
        "\u2551  \ud83c\udfaf Use `inspect_sample(idx)` to examine specific samples            \u2551\n",
        "\u2551                                                                      \u2551\n",
        "\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
        "\"\"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
