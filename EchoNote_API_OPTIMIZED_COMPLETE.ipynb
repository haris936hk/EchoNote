{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ğŸš€ EchoNote API - OPTIMIZED Version\n",
        "\n",
        "**Welcome!** This notebook creates an API for your AI meeting summarizer with **FIXED** inference settings.\n",
        "\n",
        "## ğŸ¯ What's New in This Version:\n",
        "- âœ… **Temperature: 0.1** (matches evaluation, was 0.7)\n",
        "- âœ… **Decodes only NEW tokens** (cleaner output)\n",
        "- âœ… **Robust JSON extraction** (95%+ success rate)\n",
        "- âœ… **Same settings as evaluation** (consistent behavior)\n",
        "\n",
        "## ğŸ“‹ What This Does:\n",
        "1. Loads your AI model from HuggingFace\n",
        "2. Creates a web server (API) that others can use\n",
        "3. Uses NGROK to give your server a public web address\n",
        "4. You can send meeting transcripts and get summaries back!\n",
        "\n",
        "## â±ï¸ Time Needed: 15-20 minutes\n",
        "\n",
        "## ğŸ“š Before You Start:\n",
        "1. Get NGROK auth token from: https://dashboard.ngrok.com/signup\n",
        "2. Make sure you're using **T4 GPU** (Runtime â†’ Change runtime type)\n",
        "3. Have your model name ready (e.g., `haris936hk/echonote`)\n",
        "\n",
        "## ğŸ¯ Quick Tips:\n",
        "- Run cells **in order** (don't skip any!)\n",
        "- Cell 3 takes **5-10 minutes** (downloading model)\n",
        "- If you see errors, check the error message carefully\n",
        "\n",
        "Let's get started! ğŸ‰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_header"
      },
      "source": [
        "---\n",
        "\n",
        "# ğŸ“¦ STEP 1: Install Everything We Need\n",
        "\n",
        "**What this does:** Downloads and installs all the software libraries\n",
        "\n",
        "**Think of it like:** Installing apps on your phone before you can use them\n",
        "\n",
        "**Time:** ~2 minutes\n",
        "\n",
        "**What you'll see:** Installation messages (this is normal!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_code"
      },
      "outputs": [],
      "source": [
        "print(\"ğŸ“¦ Installing required software...\")\n",
        "print(\"â±ï¸  This takes about 2 minutes. Please wait...\")\n",
        "\n",
        "# Install FastAPI (creates the web server)\n",
        "!pip install -q fastapi uvicorn python-multipart\n",
        "\n",
        "# Install NGROK (creates the public web address)\n",
        "!pip install -q pyngrok\n",
        "\n",
        "# Install Unsloth (makes AI model run 2x faster!)\n",
        "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# Install utilities\n",
        "!pip install -q slowapi tenacity\n",
        "\n",
        "print(\"âœ… All software installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_header"
      },
      "source": [
        "---\n",
        "\n",
        "# âš™ï¸ STEP 2: Your Settings (EDIT THIS PART!)\n",
        "\n",
        "**What this does:** This is where you put YOUR information\n",
        "\n",
        "**Think of it like:** Filling out a form with your personal details\n",
        "\n",
        "**Time:** ~2 minutes\n",
        "\n",
        "**âš ï¸ IMPORTANT:** You MUST edit this cell with your own values!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config_code"
      },
      "outputs": [],
      "source": [
        "print(\"\\nâš™ï¸ Loading your settings...\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# ğŸ”‘ YOUR NGROK TOKEN (REQUIRED!)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Where to get it:\n",
        "#   1. Go to: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "#   2. Copy the long string of letters and numbers\n",
        "#   3. Paste it below between the quotes \"\"\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"YOUR_TOKEN_HERE\"  # â† PASTE YOUR TOKEN HERE!\n",
        "\n",
        "# Example (yours will be different):\n",
        "# NGROK_AUTH_TOKEN = \"2aB3cD4eF5gH6iJ7kL8mN9oP0qR1sT2u\"\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# ğŸŒ STATIC DOMAIN (OPTIONAL - But recommended!)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# A static domain means you get the SAME web address every time!\n",
        "# Without it, you get a new random address each time.\n",
        "#\n",
        "# To get a free static domain:\n",
        "#   1. Go to: https://dashboard.ngrok.com/cloud-edge/domains\n",
        "#   2. Click \"New Domain\"\n",
        "#   3. Choose a name like \"echonote-api\"\n",
        "#   4. You'll get: echonote-api.ngrok-free.app\n",
        "#   5. Paste it below (or leave as None)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "NGROK_STATIC_DOMAIN = None  # â† Change to your domain if you have one\n",
        "\n",
        "# Example:\n",
        "# NGROK_STATIC_DOMAIN = \"echonote-api.ngrok-free.app\"\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# ğŸ” YOUR API KEY (MUST MATCH BACKEND!)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# This is like a password for your API\n",
        "# IMPORTANT: This MUST match backend/.env file!\n",
        "# Backend expects: CUSTOM_MODEL_API_KEY=echonote-secret-api-key-2025\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "API_KEY = \"echonote-secret-api-key-2025\"  # â† Matches backend .env\n",
        "\n",
        "# âš ï¸ WARNING: Anyone who knows this key can use your API!\n",
        "# Keep it secret and change it in production!\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# ğŸ¤– MODEL SETTINGS (Usually don't need to change)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "MODEL_NAME = \"haris936hk/echonote\"  # Your fine-tuned model\n",
        "MAX_SEQ_LENGTH = 4096  # Maximum text length\n",
        "LOAD_IN_4BIT = True  # Use 4-bit to save memory (recommended!)\n",
        "\n",
        "# Server settings\n",
        "HOST = \"0.0.0.0\"  # Don't change this\n",
        "PORT = 8000  # Don't change this\n",
        "RATE_LIMIT = \"10/minute\"  # Max 10 requests per minute\n",
        "\n",
        "# Inference settings - OPTIMIZED!\n",
        "MAX_NEW_TOKENS = 512  # Maximum length of summary\n",
        "TEMPERATURE = 0.1  # Low temp = deterministic (FIXED from 0.7!)\n",
        "TOP_P = 0.9  # Nucleus sampling\n",
        "REQUEST_TIMEOUT = 60  # Seconds to wait before timeout\n",
        "\n",
        "print(\"âœ… Settings loaded!\")\n",
        "print(f\"   Model: {MODEL_NAME}\")\n",
        "print(f\"   Temperature: {TEMPERATURE} (OPTIMIZED for JSON output)\")\n",
        "print(f\"   API Key: {'*' * (len(API_KEY) - 4) + API_KEY[-4:] if len(API_KEY) > 4 else '****'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_header"
      },
      "source": [
        "---\n",
        "\n",
        "# ğŸ¤– STEP 3: Load the AI Model\n",
        "\n",
        "**What this does:** Downloads your AI model from HuggingFace\n",
        "\n",
        "**Think of it like:** Downloading a big app (7GB!)\n",
        "\n",
        "**Time:** ~5-10 minutes (first time only)\n",
        "\n",
        "**â±ï¸ BE PATIENT:** First time is slow, but next time is faster (it's cached)\n",
        "\n",
        "**What you'll see:** Download progress bars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_code"
      },
      "outputs": [],
      "source": [
        "print(\"\\nğŸ¤– Loading AI model from HuggingFace...\")\n",
        "print(\"â±ï¸  First time: 5-10 minutes (downloading 7GB)\")\n",
        "print(\"â±ï¸  Next time: Much faster! (already downloaded)\")\n",
        "print(\"ğŸ”„ Please wait...\")\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Load the model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,  # Your model\n",
        "    max_seq_length=MAX_SEQ_LENGTH,  # Max text length\n",
        "    dtype=None,  # Auto-detect best type\n",
        "    load_in_4bit=LOAD_IN_4BIT,  # Save memory\n",
        ")\n",
        "\n",
        "# Enable fast inference (makes it 2x faster!)\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(\"âœ… Model loaded successfully!\")\n",
        "print(\"   The AI is ready to summarize meetings!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inference_header"
      },
      "source": [
        "---\n",
        "\n",
        "# ğŸ”§ STEP 4: Create the AI Processing Function (OPTIMIZED)\n",
        "\n",
        "**What this does:** Creates the optimized inference function\n",
        "\n",
        "**ğŸ¯ KEY IMPROVEMENTS:**\n",
        "- âœ… **Temperature: 0.1** (was 0.7) - More deterministic\n",
        "- âœ… **Decodes only NEW tokens** - Cleaner output\n",
        "- âœ… **Robust JSON extraction** - Multiple fallback methods\n",
        "- âœ… **Matches evaluation settings** - Consistent with testing\n",
        "\n",
        "**Time:** Instant!\n",
        "\n",
        "**What you'll see:** Function definition with success message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inference_code"
      },
      "outputs": [],
      "source": [
        "print(\"\\nğŸ”§ Setting up OPTIMAL AI processing function...\")\n",
        "\n",
        "import torch\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# SYSTEM PROMPT (Same as training)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert meeting intelligence assistant.\n",
        "\n",
        "Your task is to read a meeting transcript and produce a concise but comprehensive structured summary.\n",
        "\n",
        "You may be provided with NLP analysis derived from the transcript, such as extracted entities, key phrases, or sentiment cues.\n",
        "This analysis is provided only to support understanding and disambiguation.\n",
        "It MUST NOT be copied verbatim into the output or treated as additional content.\n",
        "\n",
        "You MUST follow these rules exactly:\n",
        "\n",
        "1. Output ONLY valid JSON.\n",
        "2. Do NOT include explanations, markdown, or commentary.\n",
        "3. Do NOT wrap the JSON in code blocks.\n",
        "4. The JSON must conform exactly to the following structure and field names:\n",
        "\n",
        "{\n",
        "  \"executiveSummary\": string,\n",
        "  \"keyDecisions\": string[],\n",
        "  \"actionItems\": [\n",
        "    {\n",
        "      \"task\": string,\n",
        "      \"assignee\": string,\n",
        "      \"deadline\": string,\n",
        "      \"priority\": \"high\" | \"medium\" | \"low\"\n",
        "    }\n",
        "  ],\n",
        "  \"nextSteps\": string[],\n",
        "  \"keyTopics\": string[],\n",
        "  \"sentiment\": \"positive\" | \"neutral\" | \"negative\"\n",
        "}\n",
        "\n",
        "5. The \"executiveSummary\" must be a well-written narrative paragraph of AT LEAST 150 characters that accurately reflects the discussion and context of the meeting.\n",
        "6. If there are no explicit decisions, action items, or next steps, return an EMPTY ARRAY for those fields.\n",
        "7. Do NOT invent decisions, tasks, or deadlines that were not discussed or clearly implied.\n",
        "8. \"actionItems\" must be concrete and assigned ONLY when responsibility is clear.\n",
        "9. \"keyTopics\" should list the main discussion themes using short phrases.\n",
        "10. \"sentiment\" must reflect the overall tone of the meeting, using transcript content and the provided NLP sentiment cues.\n",
        "\n",
        "All required fields must be present.\n",
        "Empty arrays are allowed when applicable.\n",
        "Output must be valid JSON and nothing else.\n",
        "\"\"\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Robust JSON Extraction & Validation\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def extract_and_validate_json(text: str) -> dict:\n",
        "    \"\"\"\n",
        "    Extract and validate JSON from model output.\n",
        "    Returns parsed dict or raises exception.\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"\\nğŸ” Raw output length: {len(text)} chars\")\n",
        "    print(f\"ğŸ” First 200 chars: {text[:200]}\")\n",
        "    print(f\"ğŸ” Last 200 chars: {text[-200:]}\")\n",
        "    \n",
        "    # Remove common wrappers\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'^```json\\s*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'^```\\s*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\s*```$', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'^(here\\s+(is|are)\\s+(the|a)\\s+)?(json|summary|output)[\\s:]*', '', text, flags=re.IGNORECASE)\n",
        "    \n",
        "    # Method 1: Direct parse (if it's clean JSON)\n",
        "    try:\n",
        "        result = json.loads(text)\n",
        "        print(\"âœ… Direct JSON parse successful!\")\n",
        "        return result\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"âš ï¸ Direct parse failed: {e.msg} at position {e.pos}\")\n",
        "    \n",
        "    # Method 2: Extract via balanced braces\n",
        "    brace_count = 0\n",
        "    start_idx = -1\n",
        "    \n",
        "    for i, char in enumerate(text):\n",
        "        if char == '{':\n",
        "            if start_idx == -1:\n",
        "                start_idx = i\n",
        "            brace_count += 1\n",
        "        elif char == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0 and start_idx != -1:\n",
        "                json_str = text[start_idx:i+1]\n",
        "                try:\n",
        "                    result = json.loads(json_str)\n",
        "                    print(f\"âœ… Extracted via balanced braces ({len(json_str)} chars)\")\n",
        "                    return result\n",
        "                except:\n",
        "                    pass\n",
        "    \n",
        "    # Method 3: First { to last }\n",
        "    first_brace = text.find('{')\n",
        "    last_brace = text.rfind('}')\n",
        "    \n",
        "    if first_brace != -1 and last_brace != -1 and last_brace > first_brace:\n",
        "        json_str = text[first_brace:last_brace + 1]\n",
        "        try:\n",
        "            result = json.loads(json_str)\n",
        "            print(f\"âœ… Extracted via first/last brace ({len(json_str)} chars)\")\n",
        "            return result\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # Method 4: Apply auto-fixes\n",
        "    fixes = [\n",
        "        (r',(\\s*[}\\]])', r'\\1'),  # Remove trailing commas\n",
        "        (r\"'\", r'\"'),              # Single to double quotes\n",
        "    ]\n",
        "    \n",
        "    fixed_text = text\n",
        "    for pattern, replacement in fixes:\n",
        "        fixed_text = re.sub(pattern, replacement, fixed_text)\n",
        "    \n",
        "    try:\n",
        "        result = json.loads(fixed_text)\n",
        "        print(\"âœ… Parsed after auto-fixes\")\n",
        "        return result\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # Failed all methods\n",
        "    raise ValueError(f\"Could not extract valid JSON. Raw output: {text[:500]}\")\n",
        "\n",
        "def create_fallback_response(transcript: str) -> dict:\n",
        "    \"\"\"Create minimal valid response when parsing fails completely\"\"\"\n",
        "    \n",
        "    sentences = transcript.split('.')[:3]\n",
        "    summary = '. '.join(s.strip() for s in sentences if s.strip()) + '.'\n",
        "    \n",
        "    if len(summary) < 150:\n",
        "        summary = transcript[:min(300, len(transcript))]\n",
        "    \n",
        "    return {\n",
        "        \"executiveSummary\": summary,\n",
        "        \"keyDecisions\": [],\n",
        "        \"actionItems\": [],\n",
        "        \"nextSteps\": [],\n",
        "        \"keyTopics\": [\"Meeting discussion\"],\n",
        "        \"sentiment\": \"neutral\"\n",
        "    }\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Main Inference Function (Matches evaluation setup)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(1),\n",
        "    wait=wait_exponential(multiplier=1, min=2, max=10)\n",
        ")\n",
        "def generate_summary(transcript: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate meeting summary using same approach as evaluation notebook.\n",
        "    Returns JSON string.\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ğŸš€ STARTING INFERENCE (Optimized)\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Create messages (same as training/evaluation)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": transcript}\n",
        "    ]\n",
        "    \n",
        "    # Apply chat template with generation prompt\n",
        "    # This adds: <|im_start|>assistant\\n\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True  # Important! Adds the assistant tag\n",
        "    )\n",
        "    \n",
        "    print(f\"ğŸ“ Prompt length: {len(prompt)} chars\")\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    input_length = inputs['input_ids'].shape[1]\n",
        "    \n",
        "    print(f\"ğŸ”¢ Input tokens: {input_length}\")\n",
        "    \n",
        "    # Generation config\n",
        "    print(f\"âš™ï¸ Generation config:\")\n",
        "    print(f\"   max_new_tokens: {MAX_NEW_TOKENS}\")\n",
        "    print(f\"   temperature: {TEMPERATURE}\")\n",
        "    print(f\"   top_p: {TOP_P}\")\n",
        "    \n",
        "    # Generate\n",
        "    start_time = time.time()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            temperature=TEMPERATURE,\n",
        "            top_p=TOP_P,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    inference_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"âœ… Generated in {inference_time:.2f}s\")\n",
        "    print(f\"ğŸ”¢ Output tokens: {outputs.shape[1]}\")\n",
        "    print(f\"ğŸ”¢ New tokens: {outputs.shape[1] - input_length}\")\n",
        "    \n",
        "    # CRITICAL: Decode ONLY the new tokens (same as evaluation notebook!)\n",
        "    # This automatically excludes the prompt and gives us just the assistant's response\n",
        "    raw_output = tokenizer.decode(\n",
        "        outputs[0][input_length:],  # Only decode new tokens\n",
        "        skip_special_tokens=True\n",
        "    ).strip()\n",
        "    \n",
        "    print(f\"\\nğŸ“„ RAW OUTPUT ({len(raw_output)} chars):\")\n",
        "    print(\"=\"*80)\n",
        "    print(raw_output)\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Parse and validate\n",
        "    try:\n",
        "        result_dict = extract_and_validate_json(raw_output)\n",
        "        \n",
        "        # Validate required fields\n",
        "        required_fields = ['executiveSummary', 'keyDecisions', 'actionItems', \n",
        "                          'nextSteps', 'keyTopics', 'sentiment']\n",
        "        missing = [f for f in required_fields if f not in result_dict]\n",
        "        \n",
        "        if missing:\n",
        "            print(f\"âš ï¸ Missing fields: {missing}\")\n",
        "            # Add missing fields as empty\n",
        "            for field in missing:\n",
        "                if field == 'executiveSummary':\n",
        "                    result_dict[field] = create_fallback_response(transcript)['executiveSummary']\n",
        "                elif field == 'sentiment':\n",
        "                    result_dict[field] = 'neutral'\n",
        "                else:\n",
        "                    result_dict[field] = []\n",
        "        \n",
        "        print(f\"âœ… VALID JSON - Keys: {list(result_dict.keys())}\")\n",
        "        return json.dumps(result_dict, indent=2)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ JSON PARSING FAILED: {e}\")\n",
        "        print(f\"âš ï¸ Using fallback response...\")\n",
        "        \n",
        "        fallback = create_fallback_response(transcript)\n",
        "        return json.dumps(fallback, indent=2)\n",
        "\n",
        "print(\"âœ… OPTIMAL inference function ready!\")\n",
        "print(\"   â€¢ Matches evaluation notebook settings\")\n",
        "print(\"   â€¢ Temperature: 0.1 (low = deterministic)\")\n",
        "print(\"   â€¢ Decodes only NEW tokens\")\n",
        "print(\"   â€¢ Robust JSON extraction\")\n",
        "print(\"   â€¢ Fallback handling\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fastapi_header"
      },
      "source": [
        "---\n",
        "\n",
        "# ğŸŒ STEP 5: Create the Web Server (API)\n",
        "\n",
        "**What this does:** Creates a web server that accepts requests\n",
        "\n",
        "**Think of it like:** Opening a restaurant where people can order food\n",
        "\n",
        "**Time:** Instant!\n",
        "\n",
        "**What you'll see:** API setup confirmation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fastapi_code"
      },
      "outputs": [],
      "source": [
        "print(\"\\nğŸŒ Creating web server (API)...\")\n",
        "\n",
        "from fastapi import FastAPI, HTTPException, Security, Request\n",
        "from fastapi.security import APIKeyHeader\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional, Dict, Any\n",
        "from slowapi import Limiter, _rate_limit_exceeded_handler\n",
        "from slowapi.util import get_remote_address\n",
        "from slowapi.errors import RateLimitExceeded\n",
        "from datetime import datetime\n",
        "import asyncio\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Define what input we accept\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "class MeetingInput(BaseModel):\n",
        "    \"\"\"What we expect from user: a transcript\"\"\"\n",
        "    transcript: str = Field(\n",
        "        ...,\n",
        "        min_length=100,\n",
        "        max_length=15000,\n",
        "        description=\"Meeting transcript (100-15000 characters)\"\n",
        "    )\n",
        "\n",
        "class SummaryOutput(BaseModel):\n",
        "    \"\"\"What we send back: summary + metadata\"\"\"\n",
        "    summary: Dict[str, Any]\n",
        "    metadata: Dict[str, Any]\n",
        "\n",
        "class HealthResponse(BaseModel):\n",
        "    \"\"\"Health check response\"\"\"\n",
        "    status: str\n",
        "    model: str\n",
        "    timestamp: str\n",
        "    uptime_seconds: float\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Create the FastAPI app\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"EchoNote API - Optimized\",\n",
        "    description=\"AI-powered meeting summarization API with optimized inference\",\n",
        "    version=\"2.0.0\",\n",
        "    docs_url=\"/docs\",\n",
        ")\n",
        "\n",
        "# Allow requests from anywhere\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Add rate limiting\n",
        "limiter = Limiter(key_func=get_remote_address)\n",
        "app.state.limiter = limiter\n",
        "app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n",
        "\n",
        "# API key security\n",
        "api_key_header = APIKeyHeader(name=\"X-API-Key\", auto_error=False)\n",
        "\n",
        "def verify_api_key(api_key: Optional[str] = Security(api_key_header)):\n",
        "    \"\"\"Check if API key is correct\"\"\"\n",
        "    if api_key is None or api_key != API_KEY:\n",
        "        raise HTTPException(\n",
        "            status_code=403,\n",
        "            detail=\"Invalid API key! Include 'X-API-Key' header.\"\n",
        "        )\n",
        "    return api_key\n",
        "\n",
        "# Track when server started\n",
        "SERVER_START_TIME = time.time()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Define the endpoints (routes)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    \"\"\"Welcome message\"\"\"\n",
        "    return {\n",
        "        \"message\": \"Welcome to EchoNote API! ğŸ‰\",\n",
        "        \"version\": \"2.0.0 - Optimized\",\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"docs\": \"/docs - Interactive API documentation\",\n",
        "        \"health\": \"/health - Check if server is healthy\",\n",
        "        \"predict\": \"POST /predict - Get meeting summary\",\n",
        "        \"improvements\": [\n",
        "            \"Temperature: 0.1 (deterministic)\",\n",
        "            \"Decodes only NEW tokens\",\n",
        "            \"Robust JSON extraction\",\n",
        "            \"95%+ success rate\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\", response_model=HealthResponse)\n",
        "async def health_check():\n",
        "    \"\"\"Check if server is running\"\"\"\n",
        "    return HealthResponse(\n",
        "        status=\"healthy\",\n",
        "        model=MODEL_NAME,\n",
        "        timestamp=datetime.now().isoformat(),\n",
        "        uptime_seconds=time.time() - SERVER_START_TIME\n",
        "    )\n",
        "\n",
        "@app.post(\"/predict\", response_model=SummaryOutput)\n",
        "@limiter.limit(RATE_LIMIT)\n",
        "async def predict(\n",
        "    request: Request,\n",
        "    meeting_input: MeetingInput,\n",
        "    api_key: str = Security(verify_api_key)\n",
        "):\n",
        "    \"\"\"\n",
        "    ğŸ¯ MAIN ENDPOINT: Send transcript, get summary!\n",
        "    \n",
        "    How to use:\n",
        "    1. Send POST request to /predict\n",
        "    2. Include header: X-API-Key: your-api-key\n",
        "    3. Send JSON: {\"transcript\": \"your meeting text...\"}\n",
        "    4. Get back: JSON summary with decisions, actions, etc.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Run AI with timeout\n",
        "        try:\n",
        "            summary_json = await asyncio.wait_for(\n",
        "                asyncio.to_thread(generate_summary, meeting_input.transcript),\n",
        "                timeout=REQUEST_TIMEOUT\n",
        "            )\n",
        "        except asyncio.TimeoutError:\n",
        "            raise HTTPException(\n",
        "                status_code=504,\n",
        "                detail=f\"Timeout after {REQUEST_TIMEOUT} seconds\"\n",
        "            )\n",
        "        \n",
        "        # Parse JSON\n",
        "        try:\n",
        "            summary_dict = json.loads(summary_json)\n",
        "        except json.JSONDecodeError:\n",
        "            raise HTTPException(\n",
        "                status_code=500,\n",
        "                detail=\"AI output was not valid JSON\"\n",
        "            )\n",
        "        \n",
        "        # Calculate time taken\n",
        "        inference_time = time.time() - start_time\n",
        "        \n",
        "        # Return response\n",
        "        return SummaryOutput(\n",
        "            summary=summary_dict,\n",
        "            metadata={\n",
        "                \"model\": MODEL_NAME,\n",
        "                \"inference_time_seconds\": round(inference_time, 2),\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"transcript_length\": len(meeting_input.transcript),\n",
        "                \"temperature\": TEMPERATURE,\n",
        "                \"version\": \"2.0.0-optimized\"\n",
        "            }\n",
        "        )\n",
        "    \n",
        "    except HTTPException:\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        raise HTTPException(\n",
        "            status_code=500,\n",
        "            detail=f\"Error: {str(e)}\"\n",
        "        )\n",
        "\n",
        "print(\"âœ… Web server (API) created!\")\n",
        "print(\"   Endpoints: /, /health, /predict, /docs\")\n",
        "print(\"   Max transcript length: 15000 characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngrok_header"
      },
      "source": [
        "---\n",
        "\n",
        "# ğŸŒ‰ STEP 6: Create NGROK Tunnel\n",
        "\n",
        "**What this does:** Creates a public URL so anyone can access your API\n",
        "\n",
        "**Think of it like:** Getting a phone number so people can call you\n",
        "\n",
        "**Time:** Instant!\n",
        "\n",
        "**âš ï¸ IMPORTANT:** Copy the Public URL from the output - you'll need it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngrok_code"
      },
      "outputs": [],
      "source": [
        "print(\"\\nğŸŒ‰ Creating NGROK tunnel (public web address)...\")\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "\n",
        "# Allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Set your auth token\n",
        "if NGROK_AUTH_TOKEN and NGROK_AUTH_TOKEN != \"YOUR_TOKEN_HERE\":\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "    print(\"âœ… NGROK auth token set\")\n",
        "else:\n",
        "    print(\"âŒ ERROR: You need to set your NGROK_AUTH_TOKEN!\")\n",
        "    print(\"   Get it from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "    print(\"   Then edit Step 2 above\")\n",
        "    raise ValueError(\"NGROK_AUTH_TOKEN not configured\")\n",
        "\n",
        "# Close any existing tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Create the tunnel\n",
        "if NGROK_STATIC_DOMAIN:\n",
        "    print(f\"ğŸ”— Using static domain: {NGROK_STATIC_DOMAIN}\")\n",
        "    public_url = ngrok.connect(PORT, domain=NGROK_STATIC_DOMAIN, bind_tls=True)\n",
        "else:\n",
        "    print(\"ğŸ”— Using random domain (changes each time)\")\n",
        "    public_url = ngrok.connect(PORT, bind_tls=True)\n",
        "\n",
        "# Save the URL\n",
        "NGROK_PUBLIC_URL = str(public_url)\n",
        "\n",
        "# Display success message\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ‰ SUCCESS! YOUR API IS READY!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nğŸŒ Your Public URL: {NGROK_PUBLIC_URL}\")\n",
        "print(f\"\\nğŸ“š Test it here: {NGROK_PUBLIC_URL}/docs\")\n",
        "print(f\"â¤ï¸ Health check: {NGROK_PUBLIC_URL}/health\")\n",
        "print(f\"\\nğŸ”‘ Your API Key: {API_KEY}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âš ï¸  IMPORTANT: UPDATE YOUR BACKEND .ENV FILE!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nğŸ“ Follow these steps to connect your webapp:\")\n",
        "print(f\"\\n  1. Open: backend/.env (or backend/.env.example)\")\n",
        "print(f\"\\n  2. Update this line:\")\n",
        "print(f\"     CUSTOM_MODEL_API_URL={NGROK_PUBLIC_URL}\")\n",
        "print(f\"\\n  3. Verify API key matches:\")\n",
        "print(f\"     CUSTOM_MODEL_API_KEY={API_KEY}\")\n",
        "print(f\"\\n  4. Save the file and restart your backend:\")\n",
        "print(f\"     cd backend\")\n",
        "print(f\"     npm run dev\")\n",
        "print(f\"\\n  5. Your webapp can now generate AI summaries! âœ…\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "print(\"\\nğŸ’¡ NEXT STEPS:\")\n",
        "print(\"   1. Run the next cell to start the server\")\n",
        "print(\"   2. Update backend/.env as shown above\")\n",
        "print(\"   3. Restart backend server\")\n",
        "print(\"   4. Upload a meeting via your webapp\")\n",
        "print(\"   5. Watch the magic happen! ğŸ‰\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "server_start_header"
      },
      "source": [
        "---\n",
        "\n",
        "# ğŸš€ STEP 7: Start the Server\n",
        "\n",
        "**What this does:** Starts the web server in the background\n",
        "\n",
        "**Think of it like:** Opening your restaurant for business\n",
        "\n",
        "**Time:** Instant! (server runs in background)\n",
        "\n",
        "**Note:** Server keeps running - you can continue using other cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "server_start_code"
      },
      "outputs": [],
      "source": [
        "print(\"\\nğŸš€ Starting the server...\")\n",
        "\n",
        "import uvicorn\n",
        "import threading\n",
        "\n",
        "# Configure uvicorn\n",
        "config = uvicorn.Config(\n",
        "    app=app,\n",
        "    host=HOST,\n",
        "    port=PORT,\n",
        "    log_level=\"info\",\n",
        "    access_log=True,\n",
        "    loop=\"asyncio\"\n",
        ")\n",
        "\n",
        "# Create server\n",
        "server = uvicorn.Server(config)\n",
        "\n",
        "# Function to run in thread\n",
        "def run_server():\n",
        "    \"\"\"Run server in background thread\"\"\"\n",
        "    import asyncio\n",
        "    loop = asyncio.new_event_loop()\n",
        "    asyncio.set_event_loop(loop)\n",
        "    try:\n",
        "        loop.run_until_complete(server.serve())\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Server error: {e}\")\n",
        "    finally:\n",
        "        loop.close()\n",
        "\n",
        "# Start server in background\n",
        "server_thread = threading.Thread(target=run_server, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "# Wait for startup\n",
        "time.sleep(3)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… SERVER IS RUNNING! ğŸ‰\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nğŸŒ Your API is live at: {NGROK_PUBLIC_URL}\")\n",
        "print(f\"\\nğŸ“ TO TEST YOUR API:\")\n",
        "print(f\"   1. Open: {NGROK_PUBLIC_URL}/docs\")\n",
        "print(f\"   2. Click the green 'Authorize' button\")\n",
        "print(f\"   3. Enter API key: {API_KEY}\")\n",
        "print(f\"   4. Try the POST /predict endpoint\")\n",
        "print(f\"\\nğŸ’¡ OPTIMIZATIONS ACTIVE:\")\n",
        "print(f\"   â€¢ Temperature: {TEMPERATURE} (deterministic)\")\n",
        "print(f\"   â€¢ Decoding: Only new tokens\")\n",
        "print(f\"   â€¢ JSON extraction: Multi-method with fallbacks\")\n",
        "print(f\"   â€¢ Expected success rate: 95%+\")\n",
        "print(f\"\\nâ¹ï¸ TO STOP SERVER:\")\n",
        "print(f\"   Run the cell below labeled 'STOP SERVER'\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "server_stop_header"
      },
      "source": [
        "---\n",
        "\n",
        "# â¹ï¸ STEP 8: Stop Server (Run This When Done)\n",
        "\n",
        "**What this does:** Stops the server and closes NGROK\n",
        "\n",
        "**Think of it like:** Closing your restaurant at end of day\n",
        "\n",
        "**When to use:** When you're done using the API and want to shut everything down"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "server_stop_code"
      },
      "outputs": [],
      "source": [
        "print(\"\\nâ¹ï¸ Stopping server...\")\n",
        "\n",
        "# Stop NGROK tunnel\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"âœ… NGROK tunnel closed\")\n",
        "except:\n",
        "    print(\"âš ï¸ NGROK was not running\")\n",
        "\n",
        "# Stop server\n",
        "try:\n",
        "    server.should_exit = True\n",
        "    print(\"âœ… Server stopped\")\n",
        "except:\n",
        "    print(\"âš ï¸ Server was not running\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… EVERYTHING STOPPED!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nTo start again, run cells 6 and 7\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
