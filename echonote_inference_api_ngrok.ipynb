{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ EchoNote Inference API - FastAPI + NGROK\n",
    "\n",
    "**Production-Ready API Server for Meeting Summarization**\n",
    "\n",
    "This notebook creates a FastAPI server that loads your fine-tuned EchoNote model from HuggingFace and exposes it via NGROK for remote access from localhost.\n",
    "\n",
    "## üìã Features\n",
    "\n",
    "- ‚úÖ Load model from HuggingFace Hub (haris936hk/echonote)\n",
    "- ‚úÖ FastAPI server with auto-generated Swagger docs\n",
    "- ‚úÖ NGROK tunnel with static domain support\n",
    "- ‚úÖ API Key authentication\n",
    "- ‚úÖ Rate limiting and timeout protection\n",
    "- ‚úÖ Error handling with retries\n",
    "- ‚úÖ Request logging and monitoring\n",
    "- ‚úÖ Batch inference support\n",
    "\n",
    "## üéØ Workflow\n",
    "\n",
    "```\n",
    "HuggingFace Model ‚Üí FastAPI Server ‚Üí NGROK Tunnel ‚Üí Your Localhost Client\n",
    "```\n",
    "\n",
    "## ‚ö° Quick Start\n",
    "\n",
    "1. Run all cells in order\n",
    "2. Get your NGROK URL from the output\n",
    "3. Use the URL to make API calls from localhost\n",
    "4. Visit `[NGROK_URL]/docs` for interactive API documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Core dependencies\n",
    "!pip install fastapi uvicorn python-multipart\n",
    "!pip install pyngrok\n",
    "!pip install slowapi  # Rate limiting\n",
    "\n",
    "# Model loading options (choose one based on your preference)\n",
    "# Option 1: Unsloth (Faster inference, recommended)\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# Option 2: Standard transformers (uncomment if not using unsloth)\n",
    "# !pip install transformers accelerate bitsandbytes\n",
    "\n",
    "# Utilities\n",
    "!pip install tenacity  # Retry logic\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - EDIT THESE VALUES\n",
    "# ============================================================================\n",
    "\n",
    "# HuggingFace Model\n",
    "MODEL_NAME = \"haris936hk/echonote\"  # Your fine-tuned model\n",
    "MAX_SEQ_LENGTH = 4096  # Maximum sequence length\n",
    "LOAD_IN_4BIT = True  # Memory efficient loading\n",
    "\n",
    "# NGROK Configuration\n",
    "NGROK_AUTH_TOKEN = \"YOUR_NGROK_AUTH_TOKEN\"  # Get from https://dashboard.ngrok.com\n",
    "NGROK_STATIC_DOMAIN = None  # Optional: e.g., \"your-echonote.ngrok-free.app\"\n",
    "\n",
    "# Security\n",
    "API_KEY = \"echonote-secret-api-key-2025\"  # Change this to a secure key!\n",
    "\n",
    "# Server Settings\n",
    "HOST = \"0.0.0.0\"\n",
    "PORT = 8000\n",
    "\n",
    "# Rate Limiting (requests per minute)\n",
    "RATE_LIMIT = \"10/minute\"  # Adjust based on your needs\n",
    "\n",
    "# Inference Settings\n",
    "MAX_NEW_TOKENS = 1000\n",
    "TEMPERATURE = 0.3  # Lower for more deterministic outputs\n",
    "TOP_P = 0.95\n",
    "REQUEST_TIMEOUT = 60  # Seconds\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded!\")\n",
    "print(f\"üì¶ Model: {MODEL_NAME}\")\n",
    "print(f\"üîê API Key: {'*' * (len(API_KEY) - 4) + API_KEY[-4:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ó 3. Load Model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Loading model from HuggingFace...\")\n",
    "print(f\"üì¶ Model: {MODEL_NAME}\")\n",
    "\n",
    "# Option 1: Load with Unsloth (RECOMMENDED - Faster inference)\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    \n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MODEL_NAME,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dtype=None,  # Auto-detect best dtype\n",
    "        load_in_4bit=LOAD_IN_4BIT,\n",
    "    )\n",
    "    \n",
    "    # Enable fast inference mode\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    print(\"‚úÖ Model loaded with Unsloth (fast inference enabled)!\")\n",
    "    USING_UNSLOTH = True\n",
    "    \n",
    "except ImportError:\n",
    "    # Option 2: Standard transformers (fallback)\n",
    "    print(\"‚ö†Ô∏è Unsloth not available, using standard transformers...\")\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    import torch\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        load_in_4bit=LOAD_IN_4BIT,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Model loaded with standard transformers!\")\n",
    "    USING_UNSLOTH = False\n",
    "\n",
    "# Model info\n",
    "print(f\"\\nüìä Model Information:\")\n",
    "print(f\"   - Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"   - 4-bit quantization: {LOAD_IN_4BIT}\")\n",
    "print(f\"   - Using Unsloth: {USING_UNSLOTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ 4. Define Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "# System prompt from your training\n",
    "SYSTEM_PROMPT = \"\"\"You are an AI assistant specialized in analyzing meeting transcripts.\n",
    "\n",
    "Your task is to:\n",
    "1. Read the meeting transcript carefully\n",
    "2. Analyze the NLP features provided\n",
    "3. Generate a structured JSON summary\n",
    "\n",
    "Output Format (strict JSON):\n",
    "{\n",
    "  \"executiveSummary\": string (60-100 words),\n",
    "  \"keyDecisions\": string[],\n",
    "  \"actionItems\": [\n",
    "    {\"task\": string, \"assignee\": string, \"deadline\": string, \"priority\": \"high/medium/low\"}\n",
    "  ],\n",
    "  \"nextSteps\": string[],\n",
    "  \"keyTopics\": string[],\n",
    "  \"sentiment\": \"positive\" | \"neutral\" | \"negative\"\n",
    "}\n",
    "\n",
    "Important:\n",
    "- Output ONLY valid JSON, nothing else\n",
    "- executiveSummary must be at least 150 characters\n",
    "- If no decisions/actions found, return empty arrays []\n",
    "- sentiment must match the tone of the meeting\n",
    "\"\"\"\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=10)\n",
    ")\n",
    "def generate_summary(transcript: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate meeting summary from transcript.\n",
    "    \n",
    "    Args:\n",
    "        transcript: Meeting transcript text\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with structured summary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Format prompt with chat template\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": transcript}\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # Generate\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                temperature=TEMPERATURE,\n",
    "                top_p=TOP_P,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract JSON from response (after the assistant prompt)\n",
    "        # The model should output JSON directly\n",
    "        if \"<|im_start|>assistant\" in generated_text:\n",
    "            json_output = generated_text.split(\"<|im_start|>assistant\")[-1].strip()\n",
    "        else:\n",
    "            json_output = generated_text.split(prompt)[-1].strip()\n",
    "        \n",
    "        # Remove any markdown code blocks if present\n",
    "        json_output = json_output.replace('```json', '').replace('```', '').strip()\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        logger.info(f\"‚úÖ Inference completed in {inference_time:.2f}s\")\n",
    "        \n",
    "        return json_output\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Inference error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "print(\"‚úÖ Inference function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê 5. Create FastAPI Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException, Security, Request\n",
    "from fastapi.security import APIKeyHeader\n",
    "from fastapi.responses import JSONResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import Optional, List, Dict, Any\n",
    "from slowapi import Limiter, _rate_limit_exceeded_handler\n",
    "from slowapi.util import get_remote_address\n",
    "from slowapi.errors import RateLimitExceeded\n",
    "import asyncio\n",
    "\n",
    "# ============================================================================\n",
    "# Pydantic Models (Input/Output Validation)\n",
    "# ============================================================================\n",
    "\n",
    "class MeetingInput(BaseModel):\n",
    "    \"\"\"Input model for meeting transcript\"\"\"\n",
    "    transcript: str = Field(\n",
    "        ...,\n",
    "        min_length=100,\n",
    "        max_length=10000,\n",
    "        description=\"Meeting transcript text (100-10000 characters)\"\n",
    "    )\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"transcript\": \"MEETING TRANSCRIPT:\\nOkay, let's get started. Sarah, can you give us an update on the Q3 numbers? Sure, looking at the dashboard, we're seeing revenue at $2.5M which is actually 15% above our target...\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class BatchMeetingInput(BaseModel):\n",
    "    \"\"\"Input model for batch processing\"\"\"\n",
    "    transcripts: List[str] = Field(\n",
    "        ...,\n",
    "        min_items=1,\n",
    "        max_items=10,\n",
    "        description=\"List of meeting transcripts (max 10)\"\n",
    "    )\n",
    "\n",
    "class SummaryOutput(BaseModel):\n",
    "    \"\"\"Output model for meeting summary\"\"\"\n",
    "    summary: Dict[str, Any] = Field(..., description=\"Structured meeting summary\")\n",
    "    metadata: Dict[str, Any] = Field(..., description=\"Request metadata\")\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    \"\"\"Health check response\"\"\"\n",
    "    status: str\n",
    "    model: str\n",
    "    timestamp: str\n",
    "    uptime_seconds: float\n",
    "\n",
    "# ============================================================================\n",
    "# FastAPI App Setup\n",
    "# ============================================================================\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"EchoNote Inference API\",\n",
    "    description=\"Production-ready API for meeting summarization using fine-tuned Qwen2.5-7B\",\n",
    "    version=\"1.0.0\",\n",
    "    docs_url=\"/docs\",\n",
    "    redoc_url=\"/redoc\",\n",
    ")\n",
    "\n",
    "# CORS middleware (allow all origins for development)\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Rate limiting\n",
    "limiter = Limiter(key_func=get_remote_address)\n",
    "app.state.limiter = limiter\n",
    "app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n",
    "\n",
    "# API Key authentication\n",
    "api_key_header = APIKeyHeader(name=\"X-API-Key\", auto_error=False)\n",
    "\n",
    "def verify_api_key(api_key: Optional[str] = Security(api_key_header)):\n",
    "    \"\"\"Verify API key from request header\"\"\"\n",
    "    if api_key is None or api_key != API_KEY:\n",
    "        raise HTTPException(\n",
    "            status_code=403,\n",
    "            detail=\"Invalid or missing API key. Include 'X-API-Key' header.\"\n",
    "        )\n",
    "    return api_key\n",
    "\n",
    "# Track server start time\n",
    "SERVER_START_TIME = time.time()\n",
    "\n",
    "# ============================================================================\n",
    "# Middleware for Logging\n",
    "# ============================================================================\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def log_requests(request: Request, call_next):\n",
    "    \"\"\"Log all requests with timing information\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process request\n",
    "    response = await call_next(request)\n",
    "    \n",
    "    # Calculate duration\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    # Log\n",
    "    logger.info(\n",
    "        f\"{request.method} {request.url.path} - \"\n",
    "        f\"Status: {response.status_code} - \"\n",
    "        f\"Duration: {duration:.2f}s\"\n",
    "    )\n",
    "    \n",
    "    # Add timing header\n",
    "    response.headers[\"X-Process-Time\"] = str(duration)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# ============================================================================\n",
    "# API Endpoints\n",
    "# ============================================================================\n",
    "\n",
    "@app.get(\"/\", tags=[\"General\"])\n",
    "async def root():\n",
    "    \"\"\"Root endpoint - API information\"\"\"\n",
    "    return {\n",
    "        \"message\": \"Welcome to EchoNote Inference API\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"endpoints\": {\n",
    "            \"docs\": \"/docs\",\n",
    "            \"health\": \"/health\",\n",
    "            \"predict\": \"/predict\",\n",
    "            \"batch_predict\": \"/batch-predict\"\n",
    "        },\n",
    "        \"documentation\": \"Visit /docs for interactive API documentation\"\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\", response_model=HealthResponse, tags=[\"General\"])\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return HealthResponse(\n",
    "        status=\"healthy\",\n",
    "        model=MODEL_NAME,\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        uptime_seconds=time.time() - SERVER_START_TIME\n",
    "    )\n",
    "\n",
    "@app.post(\"/predict\", response_model=SummaryOutput, tags=[\"Inference\"])\n",
    "@limiter.limit(RATE_LIMIT)\n",
    "async def predict(\n",
    "    request: Request,\n",
    "    meeting_input: MeetingInput,\n",
    "    api_key: str = Security(verify_api_key)\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate meeting summary from transcript.\n",
    "    \n",
    "    **Authentication:** Requires X-API-Key header\n",
    "    \n",
    "    **Rate Limit:** 10 requests per minute per IP\n",
    "    \n",
    "    **Returns:** Structured JSON summary with executive summary, decisions, action items, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run inference with timeout\n",
    "        try:\n",
    "            summary_json = await asyncio.wait_for(\n",
    "                asyncio.to_thread(generate_summary, meeting_input.transcript),\n",
    "                timeout=REQUEST_TIMEOUT\n",
    "            )\n",
    "        except asyncio.TimeoutError:\n",
    "            raise HTTPException(\n",
    "                status_code=504,\n",
    "                detail=f\"Request timeout after {REQUEST_TIMEOUT} seconds\"\n",
    "            )\n",
    "        \n",
    "        # Parse JSON\n",
    "        try:\n",
    "            summary_dict = json.loads(summary_json)\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"JSON parsing error: {str(e)}\")\n",
    "            logger.error(f\"Raw output: {summary_json[:500]}\")\n",
    "            raise HTTPException(\n",
    "                status_code=500,\n",
    "                detail=\"Failed to parse model output as JSON\"\n",
    "            )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Return response\n",
    "        return SummaryOutput(\n",
    "            summary=summary_dict,\n",
    "            metadata={\n",
    "                \"model\": MODEL_NAME,\n",
    "                \"inference_time_seconds\": round(inference_time, 2),\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"transcript_length\": len(meeting_input.transcript),\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {str(e)}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Internal server error: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@app.post(\"/batch-predict\", tags=[\"Inference\"])\n",
    "@limiter.limit(\"3/minute\")  # Stricter limit for batch\n",
    "async def batch_predict(\n",
    "    request: Request,\n",
    "    batch_input: BatchMeetingInput,\n",
    "    api_key: str = Security(verify_api_key)\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate summaries for multiple transcripts.\n",
    "    \n",
    "    **Authentication:** Requires X-API-Key header\n",
    "    \n",
    "    **Rate Limit:** 3 requests per minute per IP\n",
    "    \n",
    "    **Max Batch Size:** 10 transcripts\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = []\n",
    "        \n",
    "        for idx, transcript in enumerate(batch_input.transcripts):\n",
    "            try:\n",
    "                summary_json = await asyncio.to_thread(generate_summary, transcript)\n",
    "                summary_dict = json.loads(summary_json)\n",
    "                \n",
    "                results.append({\n",
    "                    \"index\": idx,\n",
    "                    \"status\": \"success\",\n",
    "                    \"summary\": summary_dict\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"index\": idx,\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            \"results\": results,\n",
    "            \"total\": len(batch_input.transcripts),\n",
    "            \"successful\": sum(1 for r in results if r[\"status\"] == \"success\"),\n",
    "            \"failed\": sum(1 for r in results if r[\"status\"] == \"error\")\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Batch prediction error: {str(e)}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Batch processing error: {str(e)}\"\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ FastAPI server configured!\")\n",
    "print(f\"üìö Endpoints: /, /health, /predict, /batch-predict\")\n",
    "print(f\"üîê Authentication: X-API-Key header required\")\n",
    "print(f\"üö¶ Rate limit: {RATE_LIMIT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê 6. Setup NGROK Tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok, conf\n",
    "import nest_asyncio\n",
    "\n",
    "# Allow nested event loops (required for Jupyter)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ============================================================================\n",
    "# NGROK Setup\n",
    "# ============================================================================\n",
    "\n",
    "# Set auth token\n",
    "if NGROK_AUTH_TOKEN and NGROK_AUTH_TOKEN != \"YOUR_NGROK_AUTH_TOKEN\":\n",
    "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "    print(\"‚úÖ NGROK auth token configured\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è NGROK_AUTH_TOKEN not set! Get yours from: https://dashboard.ngrok.com\")\n",
    "    print(\"   Update the NGROK_AUTH_TOKEN variable in section 2\")\n",
    "\n",
    "# Kill any existing tunnels\n",
    "ngrok.kill()\n",
    "\n",
    "# Configure NGROK\n",
    "conf.get_default().region = \"us\"  # Change to your region: us, eu, ap, au, sa, jp, in\n",
    "\n",
    "# Create tunnel\n",
    "if NGROK_STATIC_DOMAIN:\n",
    "    # Use static domain (free tier includes 1 static domain)\n",
    "    print(f\"üîó Creating tunnel with static domain: {NGROK_STATIC_DOMAIN}\")\n",
    "    public_url = ngrok.connect(\n",
    "        PORT,\n",
    "        domain=NGROK_STATIC_DOMAIN,\n",
    "        bind_tls=True\n",
    "    )\n",
    "else:\n",
    "    # Use random domain\n",
    "    print(f\"üîó Creating tunnel with random domain...\")\n",
    "    public_url = ngrok.connect(PORT, bind_tls=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ NGROK TUNNEL CREATED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüåê Public URL: {public_url}\")\n",
    "print(f\"üìö API Docs: {public_url}/docs\")\n",
    "print(f\"üîç Redoc: {public_url}/redoc\")\n",
    "print(f\"‚ù§Ô∏è Health Check: {public_url}/health\")\n",
    "print(f\"\\nüîê API Key: {API_KEY}\")\n",
    "print(f\"\\n‚ö° Ready to accept requests!\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store for later use\n",
    "NGROK_PUBLIC_URL = str(public_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 7. Start FastAPI Server\n",
    "\n",
    "**‚ö†Ô∏è Important:** This cell will run continuously. The server will keep running until you stop it manually.\n",
    "\n",
    "To stop the server:\n",
    "- Click the **Stop** button in the toolbar\n",
    "- Or press **Kernel ‚Üí Interrupt** in the menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "\n",
    "print(\"\\nüöÄ Starting FastAPI server...\")\n",
    "print(f\"üìç Local: http://{HOST}:{PORT}\")\n",
    "print(f\"üåê Public: {NGROK_PUBLIC_URL}\")\n",
    "print(\"\\n‚è≥ Server is running... Press STOP to terminate.\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run server\n",
    "try:\n",
    "    uvicorn.run(\n",
    "        app,\n",
    "        host=HOST,\n",
    "        port=PORT,\n",
    "        log_level=\"info\",\n",
    "        access_log=True\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nüõë Server stopped by user\")\n",
    "finally:\n",
    "    # Cleanup\n",
    "    ngrok.kill()\n",
    "    print(\"‚úÖ NGROK tunnel closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìù Testing Section\n",
    "\n",
    "**Run these cells in a SEPARATE notebook or after stopping the server above**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ 8. Test API from Localhost\n",
    "\n",
    "### Option A: Test in Browser\n",
    "\n",
    "1. Copy your NGROK URL from above\n",
    "2. Visit: `[NGROK_URL]/docs`\n",
    "3. Click \"Authorize\" and enter your API key\n",
    "4. Test the `/predict` endpoint\n",
    "\n",
    "### Option B: Test with Python Code (Run in separate notebook/terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# ============================================================================\n",
    "# UPDATE THESE FROM YOUR SERVER OUTPUT\n",
    "# ============================================================================\n",
    "NGROK_URL = \"YOUR_NGROK_URL\"  # e.g., https://your-domain.ngrok-free.app\n",
    "API_KEY = \"echonote-secret-api-key-2025\"  # Same as server\n",
    "\n",
    "# Sample transcript\n",
    "sample_transcript = \"\"\"MEETING TRANSCRIPT:\n",
    "Okay everyone, let's get started with our Q3 review. Sarah, can you walk us through the numbers?\n",
    "\n",
    "Sure thing. So looking at the dashboard, we're seeing revenue at $2.5M which is actually 15% above our target for the quarter. Our MRR is sitting at $830K with a healthy 5% month-over-month growth.\n",
    "\n",
    "That's fantastic Sarah. What about customer metrics?\n",
    "\n",
    "Customer count is at 450, up from 380 last quarter. Churn rate has dropped to 2.5% which is the lowest we've seen. NPS score is holding steady at 72.\n",
    "\n",
    "Great work team. Now, we need to discuss the product roadmap for Q4. Mike, what are we looking at?\n",
    "\n",
    "Well, we have three major initiatives. First is the mobile app redesign which should launch by October 15th. Second is the new analytics dashboard - that's about 60% complete. Third is the API v2 rollout.\n",
    "\n",
    "Okay, let's make sure we have clear ownership. Mike, you'll lead the mobile redesign. Sarah, can you take the analytics dashboard? And I'll handle the API rollout with the engineering team.\n",
    "\n",
    "Sounds good. We should also discuss the competitor analysis that came out last week.\n",
    "\n",
    "Right. The TechCrunch article about CompetitorX raising $50M is concerning. We need to accelerate our feature development to stay ahead.\n",
    "\n",
    "Agreed. Let's schedule a strategy session for next week to dive deeper into this. Everyone free Tuesday at 2pm?\n",
    "\n",
    "Works for me.\n",
    "\n",
    "Same here.\n",
    "\n",
    "Perfect. Let's wrap up with action items. Mike - mobile redesign launch plan by Friday. Sarah - analytics dashboard progress update by Wednesday. I'll send out the strategy session invite for Tuesday.\n",
    "\n",
    "Got it, thanks everyone!\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# Test Health Endpoint\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîç Testing health endpoint...\")\n",
    "try:\n",
    "    response = requests.get(\n",
    "        f\"{NGROK_URL}/health\",\n",
    "        headers={\"ngrok-skip-browser-warning\": \"true\"}\n",
    "    )\n",
    "    print(f\"‚úÖ Status: {response.status_code}\")\n",
    "    print(f\"üìÑ Response: {json.dumps(response.json(), indent=2)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {str(e)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Test Prediction Endpoint\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç Testing prediction endpoint...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    response = requests.post(\n",
    "        f\"{NGROK_URL}/predict\",\n",
    "        json={\"transcript\": sample_transcript},\n",
    "        headers={\n",
    "            \"X-API-Key\": API_KEY,\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"ngrok-skip-browser-warning\": \"true\"  # Skip NGROK interstitial page\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        \n",
    "        print(\"\\n‚úÖ SUCCESS!\")\n",
    "        print(f\"\\n‚è±Ô∏è Inference time: {result['metadata']['inference_time_seconds']}s\")\n",
    "        print(f\"\\nüìä Summary:\")\n",
    "        print(json.dumps(result['summary'], indent=2))\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n‚ùå Error: {response.status_code}\")\n",
    "        print(f\"üìÑ Response: {response.text}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Request failed: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ 9. Example: Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example batch request\n",
    "batch_transcripts = [\n",
    "    sample_transcript,\n",
    "    \"Another meeting transcript here...\",\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = requests.post(\n",
    "        f\"{NGROK_URL}/batch-predict\",\n",
    "        json={\"transcripts\": batch_transcripts},\n",
    "        headers={\n",
    "            \"X-API-Key\": API_KEY,\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"ngrok-skip-browser-warning\": \"true\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"‚úÖ Batch processing complete\")\n",
    "        print(f\"üìä Total: {result['total']}\")\n",
    "        print(f\"‚úÖ Successful: {result['successful']}\")\n",
    "        print(f\"‚ùå Failed: {result['failed']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {response.status_code} - {response.text}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Request failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Documentation & Troubleshooting\n",
    "\n",
    "### üéØ Quick Reference\n",
    "\n",
    "**API Endpoints:**\n",
    "- `GET /` - API information\n",
    "- `GET /health` - Health check\n",
    "- `POST /predict` - Single transcript inference\n",
    "- `POST /batch-predict` - Batch inference (max 10)\n",
    "- `GET /docs` - Interactive Swagger UI\n",
    "- `GET /redoc` - ReDoc documentation\n",
    "\n",
    "**Authentication:**\n",
    "```python\n",
    "headers = {\n",
    "    \"X-API-Key\": \"your-api-key\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"ngrok-skip-browser-warning\": \"true\"  # Skip NGROK warning page\n",
    "}\n",
    "```\n",
    "\n",
    "**Rate Limits:**\n",
    "- `/predict`: 10 requests/minute per IP\n",
    "- `/batch-predict`: 3 requests/minute per IP\n",
    "\n",
    "### ‚ö†Ô∏è Common Issues\n",
    "\n",
    "**1. NGROK Tunnel Not Working:**\n",
    "- ‚úÖ Make sure you set `NGROK_AUTH_TOKEN` in section 2\n",
    "- ‚úÖ Get token from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "- ‚úÖ Check if tunnel is active: run `!ngrok tunnel list`\n",
    "\n",
    "**2. API Key Errors:**\n",
    "- ‚úÖ Include `X-API-Key` header in all requests\n",
    "- ‚úÖ API key must match the one set in configuration\n",
    "\n",
    "**3. Rate Limit Errors:**\n",
    "- ‚úÖ Wait 60 seconds between batches of requests\n",
    "- ‚úÖ Use batch endpoint for multiple transcripts\n",
    "\n",
    "**4. Model Loading Errors:**\n",
    "- ‚úÖ Ensure you have sufficient GPU/RAM\n",
    "- ‚úÖ Try setting `LOAD_IN_4BIT = True` for lower memory usage\n",
    "- ‚úÖ Check model exists: https://huggingface.co/haris936hk/echonote\n",
    "\n",
    "**5. JSON Parsing Errors:**\n",
    "- ‚úÖ Model might need more examples or fine-tuning\n",
    "- ‚úÖ Check if transcript is in correct format\n",
    "- ‚úÖ Ensure transcript is between 100-10000 characters\n",
    "\n",
    "**6. Timeout Errors:**\n",
    "- ‚úÖ Increase `REQUEST_TIMEOUT` in configuration\n",
    "- ‚úÖ Shorter transcripts process faster\n",
    "- ‚úÖ Check GPU availability\n",
    "\n",
    "**7. NGROK Bandwidth Limit:**\n",
    "- ‚úÖ Free tier: 1GB/month (~1000 requests)\n",
    "- ‚úÖ Upgrade to paid plan or use alternative tunneling service\n",
    "\n",
    "### üöÄ Production Deployment\n",
    "\n",
    "**This setup is for TESTING ONLY. For production:**\n",
    "\n",
    "1. **Deploy to proper hosting:**\n",
    "   - HuggingFace Inference Endpoints\n",
    "   - AWS/GCP/Azure with proper API gateway\n",
    "   - Modal.com, Replicate, or RunPod\n",
    "\n",
    "2. **Add proper security:**\n",
    "   - JWT authentication instead of API keys\n",
    "   - HTTPS with proper SSL certificates\n",
    "   - Rate limiting per user/organization\n",
    "   - Request validation and sanitization\n",
    "\n",
    "3. **Add monitoring:**\n",
    "   - Prometheus/Grafana for metrics\n",
    "   - Sentry for error tracking\n",
    "   - CloudWatch/Datadog for logs\n",
    "\n",
    "4. **Add caching:**\n",
    "   - Redis for response caching\n",
    "   - Reduce redundant inference calls\n",
    "\n",
    "### üìö Additional Resources\n",
    "\n",
    "- **FastAPI Docs:** https://fastapi.tiangolo.com\n",
    "- **NGROK Docs:** https://ngrok.com/docs\n",
    "- **Unsloth Docs:** https://docs.unsloth.ai\n",
    "- **HuggingFace Hub:** https://huggingface.co/docs/hub\n",
    "\n",
    "### üí° Tips\n",
    "\n",
    "- üî• Use `/docs` endpoint for interactive testing\n",
    "- üìä Check `/health` endpoint to verify server is running\n",
    "- üîê Never commit API keys to git repositories\n",
    "- ‚ö° Use batch endpoint for multiple transcripts (more efficient)\n",
    "- üíæ Save important outputs immediately (NGROK sessions can disconnect)\n",
    "- üåê Get a static NGROK domain for consistent URL\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ú® What's Next?\n",
    "\n",
    "1. **Test thoroughly** with different meeting transcripts\n",
    "2. **Monitor performance** and adjust parameters\n",
    "3. **Collect feedback** on summary quality\n",
    "4. **Fine-tune further** if needed\n",
    "5. **Deploy to production** with proper infrastructure\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations! You now have a working AI-powered meeting summarization API!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
